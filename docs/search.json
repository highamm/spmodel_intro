[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Spatial Modeling with spmodel",
    "section": "",
    "text": "Preface\nThe goals of this book are to:\n\nProvide a light introduction to the concepts behind spatial modeling.\nProvide tools to fit these introductory spatial models by showing how to use the spmodel R package.\n\nThere are already many fantastic resources for learning about spatial statistics. These materials are intended for anyone making their first foray into the field. With that in mind, we introduce any necessary probability concepts (such as random variables, covariance, etc.) as well as any necessary matrix algebra concepts (such as the matrix inverse) along the way. Additionally, while we use the tidyverse suite of packages in R and we provide all code, we do not attempt to teach how to best use the tidyverse: there are already so many fantastic resources for doing so.\nBefore reading these materials, a reader should be able to fit and interpret the results from a multiple regression analysis with independent random errors. As a few examples, prior to reading these materials, a reader should be able to:\n\nInterpret the fitted slope and intercept of a linear regression model.\nState the assumptions needed for regression and how to assess the plausibility of those assumptions.\nInterpret a p-value from a hypothesis test.\nInterpret a confidence interval for one of the regression coefficients.\n\nAfter reading these materials, a reader should be able to:\n\nAssess when the use of a spatial model is appropriate.\nConstruct a plot of spatial data and interpret an empirical semivariogram.\nIdentify a few common functions used to model spatial covariance.\nExplain the difference between spatial covariance and spatial correlation.\nInterpret the following spatial covariance parameters: partial sill, range, nugget.\nWrite out the spatial linear model with correlated random errors.\nUsing spmodel’s splm() function, fit a spatial model to a data set. Using the fitted model, obtain and interpret:\n\na summary table of model output, obtained with tidy().\na summary table of covariance parameter estimates, obtained with tidy().\nmodel fit statistics, obtained with glance() and glances().\nfitted values and residuals, obtained with augment().\n\nState and assess the assumptions of a spatial linear model.\nConstruct a prediction for the response variable at an unobserved spatial location.\nExplain some of the intuition behind how spatial information is used to construct predictions.\nIdentify an appropriate situation to use a spatial logistic regression model.\nFit and interpret a spatial logistic regression model.\nExplain the difference between spatial point data and spatial polygon data.\nFit and interpret a spatial autoregressive model to spatial polygon data."
  },
  {
    "objectID": "data-exploration.html#introduction-to-spatial-data",
    "href": "data-exploration.html#introduction-to-spatial-data",
    "title": "1  Spatial Data Exploration",
    "section": "1.1 Introduction to Spatial Data",
    "text": "1.1 Introduction to Spatial Data\nWhat exactly makes a spatial analysis an appropriate choice? All data is collected at some location in space, but spatial analysis is useful if we expect that observations that are collected closer together in space are going to be more similar than observations that are collected further apart in space.\nFor example, suppose that we have data on sulfate atmospheric deposition (in kg per hectare) in the United States at 197 unique locations in Alaska. In this example, we expect there to be some spatial correlation: sulfate depositions should be more similar if they are collected at locations that are near one another. In this example, a spatial analysis of the sulfate data is appropriate.\nIn contrast, suppose that we have data on whether or not a professional tennis player wins tournaments that they play throughout the world in a calendar year. While this data is also collected in space (at the courts where the player plays their tournaments), a spatial analysis is less appropriate here. We would expect whether or not the player wins the tournament to be strongly driven by factors like the court surface and the level of the tournament, but we would expect little spatial correlation.\nWhether or not spatial analysis is appropriate is not always an easy question. Throughout these course materials, we use examples from fields where spatial analysis is more common, including examples using ecological data, environmental science data, and housing data.\n\nFor our first example, consider again the sulfate data described above. The data, called sulfate and stored as a “simple features” sf object, can be loaded with\n\nsulfate\n## Simple feature collection with 197 features and 1 field\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -2292550 ymin: 386181.1 xmax: 2173345 ymax: 3090370\n## Projected CRS: NAD83 / Conus Albers\n## First 10 features:\n##    sulfate                 geometry\n## 1   12.925 POINT (817738.8 1080571)\n## 2   20.170 POINT (914593.6 1295545)\n## 3   16.822 POINT (359574.1 1178228)\n## 4   16.227 POINT (265331.9 1239089)\n## 5    7.858 POINT (304528.8 1453636)\n## 6   15.358 POINT (162932.8 1451625)\n## 7    0.986 POINT (-1437776 1568022)\n## 8    0.425 POINT (-1572878 1125529)\n## 9    3.585 POINT (-1282009 1204889)\n## 10   2.383 POINT (-1972775 1464991)\n\nAn sf object is a convenient way to store spatially indexed data in R. Examining our sf object, we can break down the output line-by-line:\n\nthe first line tells us that there are 197 “features” and 1 “field”: this means that there are 197 spatial locations and 1 variable that has been collected at those spatial locations.\nthe POINT Geometry type means that the field is recorded at specific points. Another Geometry type is POLYGON, which means that each feature is collected at a polygonal area.\nthe Dimension, Bounding box, and Projected CRS (Coordinate Reference System) give some additional spatial information. We will discuss the CRS further in Section 1.3.3.\nFinally, the “First 10 features” show the first 10 spatial locations. This part of the output should look familiar to anyone who uses tibbles. The primary difference is that there is a column specifically named geometry that gives the spatial location of each feature. In this case, since the Geometry type is POINT and the Dimension is XY, geometry gives the X and Y point coordinates of each feature.\n\nIn spatial statistics, we should construct a plot of the response variable at the measured spatial locations. Much like other subfields of statistics, plotting our data is an important first step toward a more formal analysis! We can make a plot of the sulfate variable with\n\nggplot(data = sulfate, aes(colour = sulfate)) +\n  geom_sf() +\n  scale_colour_viridis_c() +\n  theme_void() +\n  labs(colour = \"Sulfate (kg per ha)\")\n\n\n\n\nNote that, because sulfate is an sf object, we do not need to specify x and y coordinates for our plot: these are pulled automatically from the geometry of sulfate when we use the geom_sf() function designed specifically for sf data objects.\nBased on the plot, is the sulfate variable spatially correlated? To make an informal assessment, the question we ask ourselves is generally “Are sulfate values more similar for locations closer together than they are for locations further apart?” For this example, we do see that, in general, locations closer together have more similar sulfate values. The northeast region of the United States has locations that tend to have high sulfate while the western region United States has locations that tend to have lower sulfate.\nBased on the map, we might say that sulfate is “very” spatially correlated. But, “very” is a vague adjective based only on our own subjective assessment. How might we explore the nature of the spatial correlation with a different plot?"
  },
  {
    "objectID": "data-exploration.html#the-empirical-semivariogram",
    "href": "data-exploration.html#the-empirical-semivariogram",
    "title": "1  Spatial Data Exploration",
    "section": "1.2 The Empirical Semivariogram",
    "text": "1.2 The Empirical Semivariogram\nWhile the plot of the response variable with x and y-coordinates is a useful initial plot for examining spatial data, we might want to contruct a plot to more clearly assess the degree of spatial correlation for a response variable. One common plot made to explore the degree of spatial correlation is the empirical semivariogram. Before building the empirical variogram for the sulfate variable, we will first build an empirical variogram “from scratch” using a small toy data set consisting of just 4 observations:\n\ntoy_df &lt;- tibble(obs_id = c(\"A\", \"B\", \"C\", \"D\"),\n                 xcoord = c(1, 1, 2, 2), ycoord = c(1, 2, 1, 2),\n                 z = c(9, 7, 6, 1))\ntoy_df\n## # A tibble: 4 × 4\n##   obs_id xcoord ycoord     z\n##   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 A           1      1     9\n## 2 B           1      2     7\n## 3 C           2      1     6\n## 4 D           2      2     1\n\nThere are four variables in toy_df:\n\nobs_id, an observation identification letter.\nxcoord and ycoord, which give the spatial coordinates of each observation.\nz, a generic response variable collected at each of the four spatial locations.\n\nThe x-axis of an empirical semivariogram is the distance, denoted \\(h\\), between data points. The y-axis of an empirical semivariogram is the semivariance, defined as the average squared difference between values of z for all pairs of locations with the distance \\(h\\), divided by 2. The semivariance, denoted \\(\\gamma\\), is a function of the distance, \\(h\\), so it is often written as \\(\\gamma(h)\\). For example, the following pairs of observations are a distance of \\(1\\) unit apart:\n\nA & B.\nA & C.\nB & D.\nC & D.\n\nWe take the average squared difference in z for each of these pairs and divide the result by 2 to obtain the semivariance value for the semivariogram at the distance \\(h = 1\\):\n\\[\n\\gamma(1) = \\frac{1}{2} \\cdot \\frac{(9 - 7)^2 + (9 - 6)^2 +(7 - 1)^2 + (6 - 1)^2}{4} = 9.25.\n\\]\nWe then repeat this calculation for all unique distances. The only other unique distance left in this small data set is the distance between A & D and B & C, both of which are 1.414 units apart. So, the semivariance at distance \\(h = 1.414\\) is\n\\[\n\\gamma(1.414) = \\frac{1}{2} \\cdot \\frac{(9 - 1)^2 + (7 - 6)^2}{2} = 16.25.\n\\]\nOur toy empirical semivariogram for this very small example looks like:\n\n\n\n\n\nFor almost any practical example, there are far more pairs of data points than the 6 pairs we used for the toy empirical semivariogram. And, if the points are not in a grid, there are many distances that only have one unique pair. For example, in the sulfate data with just 197 spatial locations, there are 19307 unique distance values between points. Therefore, semivariograms are rarely constucted using all unique distances, as was done with the toy example. Instead, empirical semivariograms are almost always constructed by binning distances and calculating the semivariance for all pairs of observations that have a distance that falls into the bin.\n\n\n\n\n\n\nNote\n\n\n\nThe binning of distances to create the empirical semivariogram is analagous to the binning of a quantitative variable to create a standard histogram. In both cases, bins usually have same “width”, and, in both cases, choosing the number of bins will slightly change the way that the resulting plot looks.\n\n\nA formula for the semivariance is:\n\\[\n\\gamma(h) = \\frac{1}{2\\cdot \\vert N(h) \\vert }\\sum_{N(h)}(y_i - y_j)^2,\n\\tag{1.1}\\]\nwhere \\(N(h)\\) is the set of all pairs that fall within a particular distance bin, \\(\\vert N(h) \\vert\\) is the total number of such pairs, and \\(y_i\\) and \\(y_j\\) denote the response variable for a particular pair of locations \\(i\\) and \\(j\\).\nTo construct an empirical semivariogram of the sulfate variable, we can use the esv() function from spmodel to perform the binning and calculate the semivariance for all pairs of observations within each distinct bin. The esv() function has two required arguments:\n\ndata, either a data.frame, tibble, or sf object and\nformula, a formula that gives the response variable and any predictor variables.\n\nIn this example, we do not have any predictors so the right-hand-side of the formula argument is 1.\n\nspmodel::esv(formula = sulfate ~ 1, data = sulfate)\n##                   bins      dist     gamma   np\n## 1          (0,1.5e+05]  103340.3  18.04594  149\n## 2   (1.5e+05,3.01e+05]  232013.8  20.28099  456\n## 3  (3.01e+05,4.51e+05]  379254.7  27.63260  749\n## 4  (4.51e+05,6.02e+05]  529542.7  31.65651  887\n## 5  (6.02e+05,7.52e+05]  677949.1  43.28972  918\n## 6  (7.52e+05,9.03e+05]  826916.7  41.26845 1113\n## 7  (9.03e+05,1.05e+06]  978773.3  46.58159 1161\n## 8   (1.05e+06,1.2e+06] 1127232.1  51.05177 1230\n## 9   (1.2e+06,1.35e+06] 1275414.7  58.81009 1239\n## 10  (1.35e+06,1.5e+06] 1429183.9  71.88921 1236\n## 11  (1.5e+06,1.65e+06] 1577636.1  79.03967 1139\n## 12 (1.65e+06,1.81e+06] 1729098.3  94.49986 1047\n## 13 (1.81e+06,1.96e+06] 1879678.7  99.49936  934\n## 14 (1.96e+06,2.11e+06] 2029566.3 113.57088  842\n## 15 (2.11e+06,2.26e+06] 2181336.7 125.05567  788\n\nThe output from esv() is a data frame with\n\nbins, a variable that gives the bins. By default, the bins are all of equal width (as are the bins in a standard histogram of a quantitative variable).\ndist is the average distance, in meters, of all of the pairs that fall within the bin.\ngamma is the value of the semivariance, \\(\\gamma(h)\\), according to Equation 1.1.\nnp is the number of points, \\(\\vert N(h) \\vert\\) from Equation 1.1.\n\nFor example, the bin from 0 meters to 150000 meters contains all pairs of locations that are between 0 and 150000 meters apart. For each of these 149 pairs, we take the difference in sulfate values, square them, and divide by \\(149 \\cdot 2\\) to obtain the value for the semivariance (called gamma in the output) of 18.0459.\nWe can then plot the semivariogram for the sulfate variable, where a point in the plot is larger when there are more pairs of distances in the bin, with\n\nsemivar_df &lt;- spmodel::esv(sulfate ~ 1, data = sulfate)\n\nggplot(data = semivar_df, aes(x = dist, y = gamma, size = np)) +\n  geom_point() +\n  ylim(c(0, NA)) +\n  theme_minimal() +\n  labs(x = \"Distance (meters)\")\n\n\n\n\n\n1.2.1 Interpreting the Empirical Semivariogram\nIn general, a semivariogram with an upward trend indicates that there is spatial correlation in the response variable. Why is this the case? If there is spatial correlation in the response variable, then pairs of observations with smaller distances (that are closer together) will tend to have response variable measurements that are more similar, which leads to smaller squared differences in the variable of interest, which leads to a smaller semivariance value. On the other hand, pairs of observations with larger distances (that are further apart) will tend to have response variable measurements that are less similar, which leads to larger squared differences in the response variable, which leads to a larger semivariance value.\nWe see that there is indeed evidence of a lot of spatial correlation in the sulfate variable: the semivariance is quite small for smaller distances and increases for larger distances. Unlike histograms, where the bins are explicitly shown, empirical semivariograms typically only show points corresponding to the semivariance at different distances. In this example, the total number of bins is 15.\nA semivariogram without an upward trend indicates little to no spatial correlation; in this case, the average squared differences are similar no matter what the distance between the points is. A semivariogram with a downward trend is more rare; such a semivariogram would indicate that locations closer together tend to be less similar (with values for the variable of interest that are very different from one another) than locations that are further apart.\nBecause of the squared difference term in the numerator of Equation 1.1, the calculation of the empirical semivariance is sensitive to large outliers. Therefore, in addition to a spatial map of a variable and an empirical semivariogram, exploration of the response variable with a standard histogram (or other graph for exploring a single quantitative variable) is useful. In general, for sf objects, we can use geoms like geom_histogram() in the same way that we use them for data.frame objects and/or tibbles:\n\nggplot(data = sulfate, aes(x = sulfate)) +\n  geom_histogram(colour = \"black\", fill = \"white\", bins = 15) +\n  theme_minimal() +\n  labs(x = \"Sulfate (kg per ha)\")\n\n\n\n\nHere, we see that the distribution of the sulfate variable is moderately right-skewed, but there are no extreme outliers present. If there was an extreme outlier, we might consider making an empirical semivariogram with the outlier in the data and without the outlier so that we can determine if the outlier has a strong effect on the plot."
  },
  {
    "objectID": "data-exploration.html#more-on-sf-objects",
    "href": "data-exploration.html#more-on-sf-objects",
    "title": "1  Spatial Data Exploration",
    "section": "1.3 More on sf Objects",
    "text": "1.3 More on sf Objects\nSo far, we have explored the sulfate data with both a plot of the sulfate variable at the 197 locations where it was collected and an empirical semivariogram plot, which is a commonly used exploratory plot for spatial data. We now turn our attention to a very short introduction of sf (simple features) objects, we will use quite heavily throughout these materials. In this subsection, we will gain a little bit more familiarity with these objects. However, there is a lot more to learn about the sf package not presented here, and we encourage any interested reader to examine the sf package vignettes found at https://r-spatial.github.io/sf/articles/sf1.html.\nThe sf package in R can store objects with spatial information as sf simple feature objects. While we will not review simple feature objects or the sf package in depth, we will discuss a couple of important components of sf objects.\n\n1.3.1 Geometry Type\nThe two geometry types that we will encounter throughout these materials are POINT geometries and POLYGON geometries, though other types can be found at https://r-spatial.github.io/sf/articles/sf1.html#simple-feature-geometry-types.\nThe POLYGON geometry type provides polygonal boundaries for each spatial location. An example of an sf object with POLYGON geometry is the seal data in the spmodel package:\n\nseal\n## Simple feature collection with 62 features and 1 field\n## Geometry type: POLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 913618.8 ymin: 1007542 xmax: 1116002 ymax: 1145054\n## Projected CRS: NAD83 / Alaska Albers\n## # A tibble: 62 × 2\n##   log_trend                                                             geometry\n##       &lt;dbl&gt;                                                        &lt;POLYGON [m]&gt;\n## 1  NA       ((1035002 1054710, 1035002 1054542, 1035002 1053542, 1035002 105254…\n## 2  -0.282   ((1037002 1039492, 1037006 1039490, 1037017 1039492, 1037035 103949…\n## 3  -0.00121 ((1070158 1030216, 1070185 1030207, 1070187 1030207, 1070211 103020…\n## 4   0.0354  ((1054906 1034826, 1054931 1034821, 1054936 1034822, 1055001 103482…\n## 5  -0.0160  ((1025142 1056940, 1025184 1056889, 1025222 1056836, 1025256 105678…\n## 6   0.0872  ((1026035 1044623, 1026037 1044605, 1026072 1044610, 1026083 104461…\n## # ℹ 56 more rows\n\nThe geometry column shows a series of points for each row; these points can be connected with line segments to form a polygon. Data on these seals were collected in polygon areas, not at specific points, so a POLYGON geometry makes more sense for this data.\n\n\n1.3.2 Converting data.frame to sf\nIf we have a data.frame object that we wish to convert to an POINT referenced sf object, we can use the st_as_sf() function from the sf package. For example, the caribou data frame in spmodel is of class data.frame, but has columns for spatial coordinates called x and y:\n\ncaribou\n## # A tibble: 30 × 5\n##   water tarp      z     x     y\n##   &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 Y     clear  2.42     1     6\n## 2 Y     shade  2.44     2     6\n## 3 Y     none   1.81     3     6\n## 4 N     clear  1.97     4     6\n## 5 N     shade  2.38     5     6\n## 6 Y     none   2.22     1     5\n## # ℹ 24 more rows\n\nWe can convert the caribou data frame to an sf object using the st_as_sf() function from the sf package, providing the column names for the spatial coordinates as an argument to coords:\n\ncaribou_sf &lt;- caribou |&gt; sf::st_as_sf(coords = c(\"x\", \"y\"))\ncaribou_sf\n## Simple feature collection with 30 features and 3 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: 1 ymin: 1 xmax: 5 ymax: 6\n## CRS:           NA\n## # A tibble: 30 × 4\n##   water tarp      z geometry\n##   &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt;  &lt;POINT&gt;\n## 1 Y     clear  2.42    (1 6)\n## 2 Y     shade  2.44    (2 6)\n## 3 Y     none   1.81    (3 6)\n## 4 N     clear  1.97    (4 6)\n## 5 N     shade  2.38    (5 6)\n## 6 Y     none   2.22    (1 5)\n## # ℹ 24 more rows\n\nNow, caribou_sf, has a geometry column and is of class sf:\n\nclass(caribou_sf)\n## [1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nNote that, in most cases, we can use dplyr functions on sf objects in the same way we use them on data.frame objects or tibble objects because sf objects are also of class data.frame and tbl:\n\ncaribou |&gt; filter(water == \"Y\")\ncaribou_sf |&gt; filter(water == \"Y\")\n\ncaribou |&gt; mutate(water_tarp = interaction(water, tarp))\ncaribou_sf |&gt; mutate(water_tarp = interaction(water, tarp))\n\n\n\n1.3.3 CRS\nFinally, an sf object has a Coordinate Reference System (CRS). A coordinate reference system is used to project the data collected on Earth’s sphere to a 2-dimensional plane. A common CRS for data collected in the United States is the NAD83 / Conus Albers projection. This projection is what is used for the sulfate data:\n\nsulfate\n## Simple feature collection with 197 features and 1 field\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -2292550 ymin: 386181.1 xmax: 2173345 ymax: 3090370\n## Projected CRS: NAD83 / Conus Albers\n## First 10 features:\n##    sulfate                 geometry\n## 1   12.925 POINT (817738.8 1080571)\n## 2   20.170 POINT (914593.6 1295545)\n## 3   16.822 POINT (359574.1 1178228)\n## 4   16.227 POINT (265331.9 1239089)\n## 5    7.858 POINT (304528.8 1453636)\n## 6   15.358 POINT (162932.8 1451625)\n## 7    0.986 POINT (-1437776 1568022)\n## 8    0.425 POINT (-1572878 1125529)\n## 9    3.585 POINT (-1282009 1204889)\n## 10   2.383 POINT (-1972775 1464991)\n\nWe will not discuss the details of choosing an appropriate Coordinate Reference System here, but the Modern Data Science with R textbook provides a brief introduction to Coordinate Reference Systems: https://mdsr-book.github.io/mdsr2e/ch-spatial.html#sec:projections. Though we do not have the space to discuss CRS’s here, choosing an appropriate CRS for your data is quite important. Much of the data that we use in future sections is data collected in the United States, where the most commonly used CRS is based off of the NAD83 / Albers projection.\nTo show why choosing an appropriate CRS is important, we can observe how a different projection might distort spatial locations by comparing a the NAD83 / Albers projection of the sulfate data with the WGS 84 / Pseudo-Mercator projection:\n\n## define a different CRS for sulfate data\nsulfate_wgs &lt;- sf::st_transform(sulfate, crs = 3857) \n\n## original (good) projection\nggplot(data = sulfate, aes(colour = sulfate)) +\n  geom_sf() +\n  theme_void() +\n  scale_colour_viridis_c()\n\n\n\n\n## new (bad) projection\nggplot(data = sulfate_wgs, aes(colour = sulfate)) +\n  geom_sf() +\n  theme_void() +\n  scale_colour_viridis_c()\n\n\n\n\nWe see that, for the new (inappropriate) projection, the sulfate locations now look different, with the locations in the northeastern United States getting “stretched” so that the coordinates bare much less resemblance to the continental United States."
  },
  {
    "objectID": "spatial-covariance.html#exploring-the-baltimore-housing-data",
    "href": "spatial-covariance.html#exploring-the-baltimore-housing-data",
    "title": "2  Spatial Covariance and Correlation",
    "section": "2.1 Exploring the Baltimore Housing Data",
    "text": "2.1 Exploring the Baltimore Housing Data\nThroughout this section, we use the baltimore data set from the spData package. This data set contains information on 211 house prices in the Baltimore area in the year 1978. While there are many variables collected on the 211 houses, we will focus only on the following 3 variables in this section:\n\nPRICE, the price of the home, in thousands of dollars.\nX, the x-coordinate location of the home (with an unknown projection).\nY, the y-coordinate location of the home (with an unknown projection).\n\nNote that, because the projection is unknown, we do not know the exact units of distance between two homes in the data set. But, we do have the relative spatial coordinates of each home with unknown units.\nBefore we dive into modeling spatial covariance, we will first explore the baltimore data set using some plots from the previous section.\nWe can convert the baltimore data frame object to an sf object with the st_as_sf() function from the sf package:\n\nbaltimore_sf &lt;- baltimore |&gt; st_as_sf(coords = c(\"X\",\"Y\"), remove = FALSE)\nbaltimore_sf\n## Simple feature collection with 211 features and 17 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: 860 ymin: 505.5 xmax: 987.5 ymax: 581\n## CRS:           NA\n## First 10 features:\n##    STATION PRICE NROOM DWELL NBATH PATIO FIREPL AC BMENT NSTOR GAR AGE CITCOU\n## 1        1  47.0     4     0   1.0     0      0  0     2     3   0 148      0\n## 2        2 113.0     7     1   2.5     1      1  1     2     2   2   9      1\n## 3        3 165.0     7     1   2.5     1      1  0     3     2   2  23      1\n## 4        4 104.3     7     1   2.5     1      1  1     2     2   2   5      1\n## 5        5  62.5     7     1   1.5     1      1  0     2     2   0  19      1\n## 6        6  70.0     6     1   2.5     1      1  0     3     3   1  20      1\n## 7        7 127.5     6     1   2.5     1      1  1     3     1   2  20      1\n## 8        8  53.0     8     1   1.5     1      0  0     0     3   0  22      1\n## 9        9  64.5     6     1   1.0     1      1  1     3     2   0  22      1\n## 10      10 145.0     7     1   2.5     1      1  1     3     2   2   4      1\n##     LOTSZ  SQFT   X   Y        geometry\n## 1    5.70 11.25 907 534 POINT (907 534)\n## 2  279.51 28.92 922 574 POINT (922 574)\n## 3   70.64 30.62 920 581 POINT (920 581)\n## 4  174.63 26.12 923 578 POINT (923 578)\n## 5  107.80 22.04 918 574 POINT (918 574)\n## 6  139.64 39.42 900 577 POINT (900 577)\n## 7  250.00 21.88 918 576 POINT (918 576)\n## 8  100.00 36.72 907 576 POINT (907 576)\n## 9  115.90 25.60 918 562 POINT (918 562)\n## 10 365.07 44.12 897 576 POINT (897 576)\n\nThe remove = FALSE argument says to keep the \"X\" and \"Y\" coordinate columns in the data frame, in addition to using them to create a new geometry column.\nTo explore the PRICE variable in the baltimore_sf data, we will make a plot of PRICE on the spatial coordinates, an empirical semivariogram of PRICE, and a basic histogram of PRICE:\n\n## plot on spatial coordinates\nggplot(data = baltimore_sf, aes(colour = PRICE)) +\n  geom_sf() +\n  scale_colour_viridis_c() +\n  theme_void()\n\n\n\n\n\n## empirical semivariogram\nbaltimore_esv &lt;- esv(PRICE ~ 1, data = baltimore_sf)\n\nggplot(data = baltimore_esv, aes(x = dist, y = gamma, size = np)) +\n  geom_point() +\n  lims(y = c(0, NA)) \n\n\n\n\n\n## basic histogram\nggplot(data = baltimore_sf, aes(x = PRICE)) +\n  geom_histogram(colour = \"black\", fill = \"white\", bins = 15)\n\n\n\n\nWe can see a few things from our exploratory plots. First, from the first two plots, we see that there is spatial correlation in the housing PRICEs. For example, in the plot of PRICE on the x and y coordinates, houses in the northern region of the study area tend to priced higher while houses priced in the southern area of the study area tend to have lower prices. We also see an increasing pattern in the empirical semivariogram, further indicating that houses closer together in space tend to have more similar prices than houses that are further apart.\nFrom the histogram of PRICE, we see that, while there is some right-skewness, the skewness is fairly mild. We also see that, while there are some outliers with somewhat high prices, these outliers are not too extreme. The units of PRICE are in hundreds of thousands of dollars: the house prices are so low because the data was collected in 1978!\nAfter exploring our data set, we now want to more formally model the spatial correlation that we see in the plot with the spatial coordinates and the empirical semivariogram. To do so, we first need to define the terms random variable, spatial covariance, and spatial correlation."
  },
  {
    "objectID": "spatial-covariance.html#correlation-and-covariance",
    "href": "spatial-covariance.html#correlation-and-covariance",
    "title": "2  Spatial Covariance and Correlation",
    "section": "2.2 Correlation and Covariance",
    "text": "2.2 Correlation and Covariance\nUp until now, we have been using the term “spatial correlation” without actually explicitly defining the term. We have some intuition that there is spatial correlation if units that are closer together in proximity are more similar than units that are further apart. However, to explicitly model spatial correlation, we need to explicitly define the term.\n\nFirst, we will introduce the concept of a random variable, which we are informally defining as a numeric variable whose possible values that can be expressed with some probability distribution. Random variables are often denoted with a capital letter \\(Y\\) or a capital letter \\(X\\).\nAs a non-spatial example, we can define the random variable \\(Y\\) as the number of Heads in \\(n = 10\\) flips of a fair coin. While \\(Y\\) itself is inherently random and never observed, we can obtain a single realization of \\(Y\\) by actually conducting the coin-flip experiment and observing the number of heads. Suppose that we do conduct this experiment, observing 4 Heads. The realization of the random variable \\(Y\\) is often denoted with a lower-case \\(y\\). In this example, \\(y = 4\\). If we repeat the coin flip experiment, possible values for \\(y\\) are \\(y = 0, y = 1, \\ldots y = 10\\).\nMoving from the coin-flip example to a spatial example with the Baltimore housing data set, we can imagine that the house prices at all 211 locations in the data set are generated from random variables, \\(Y_1, Y_2, \\ldots, Y_{211}\\). We can think of our data as a single realization of these random variables, \\(y_1 = 47.0, y_2 = 113.0, \\ldots, y_{211} = 29.5\\).\n\n2.2.1 Introducing Spatial Correlation\nSo, why introduce random variables before we discuss spatial correlation. The reason is that when we think about spatial correlation in house prices, we always mean correlation between the random variables for house price, not the observed values (which, after they are observed, are fixed and no longer random). The definition of correlation between two random variables, \\(Y_1\\) with mean \\(\\mu_1\\) and standard deviation \\(\\sigma_1\\) and \\(Y_2\\) with mean \\(\\mu_2\\) and standard deviation \\(\\sigma_2\\) is:\n\\[\n\\text{corr}(Y_1, Y_2) = \\frac{\\text{E}[(Y_1 - \\mu_1)(Y_2 - \\mu_2)]}{\\sigma_1 \\sigma_2},\n\\tag{2.1}\\]\nwhere \\(\\text{E}((Y_1 - \\mu_1)(Y_2 - \\mu_2))\\) denotes the expected value, or, the long-run average in the quantity \\((Y_1 - \\mu_1)(Y_2 - \\mu_2)\\). The correlation between two random variables is always between \\(-1\\) and \\(1\\), inclusive. If this formula is a lot to take in, do not worry about it too much: spatial correlation is commonly modeled with a particular spatial correlation function and we will rarely make use of the definition of correlation explicitly.\nIf the correlation between random variables \\(Y_1\\) and \\(Y_2\\) is positive, then, if were to theoretically obtain thousands of realizations of \\(Y_1\\) and \\(Y_2\\) and plot these thousands of realizations on a scatterplot (with the realizations of \\(Y_2\\) on the y-axis and the realizations of \\(Y_1\\) on the x-axis), then we would observe an increasing trend. In other words, the realizations of \\(Y_1\\) and \\(Y_2\\) would tend to be larger together and would tend to be smaller together. If the correlation between random variables \\(Y_1\\) and \\(Y_2\\) is negative, then, if were to theoretically obtain thousands of realizations of \\(Y_1\\) and \\(Y_2\\) and plot these thousands of realizations on a scatterplot, then we would observe a decreasing trend.\nWe have quickly glossed over a couple of things in the previous definition of correlation because, for introductory spatial models, we will not actually use the definition of correlation explicitly. Instead, for spatial models, we assume that the correlation between two random variables, \\(Y_1\\) and \\(Y_2\\), at spatial locations \\(1\\) and \\(2\\), can be modeled as a function of the spatial distance, denoted \\(h_{12}\\), between the locations where \\(y_1\\) and \\(y_2\\) are observed.\nAn example may help with an understanding of spatial correlation. A common spatial correlation function is the exponential function:\n\\[\n\\text{Corr}(Y_1, Y_2) = e^{\\frac{-h_{12}}{\\phi}},\n\\tag{2.2}\\]\nwhere \\(\\phi &gt; 0\\), commonly called the range parameter, controls the rate of decay in the correlation. A larger \\(\\phi\\) value would indicate that the correlation decays slowly with distance while a smaller \\(\\phi\\) value would indicate that the correlation decays quickly with distance.\nThere are a few things we should note about the exponential correlation function. First, if we use this correlation function, \\(\\text{Corr}(Y_1, Y_2)\\) must be between 0 and 1. Unlike the general definition of correlation given in Equation 2.1 (where the correlation can be negative), many spatial correlation functions, including the exponential, only allow non-negative correlation in the random variables because negative correlation is rarely observed in real spatial data. In other words, it is rare for locations close together in space to be less similar than locations further apart.\nSecond, the idea of spatial correlation is a little abstract. In our baltimore house data, and, in most spatial applications, we only have one realization of our random variables. In other words, we only have one realization of the random variable \\(Y_1\\), which is \\(y_1 = 47.0\\); we only have one realization of the random variable \\(Y_2\\), which is \\(y_2 = 113.0\\), etc. Understanding the idea of spatial correlation requires that we think about a “what if” scenario: what if we were able to get millions of realizations of \\(Y_1\\) and \\(Y_2\\)? How would these realizations vary together?\nThinking again about our Baltimore housing example, we can estimate \\(\\phi\\), the range parameter that controls how quickly spatial correlation decays with increased distance, and then determine the correlation between the random variables for house price at two spatial locations in the data set. We will talk more about how to estimate \\(\\phi\\) later in this section, but, for now, suppose that we obtain an estimate of 10.6.\nConsider the three spatial locations shown on the plot below. The distance between location 1 and location 2 is 42.72 units while the distance between location 2 and location 3 is 7.28 units.\n\nbaltimore_label &lt;- baltimore_sf |&gt; slice(1:3)\nggplot(data = baltimore_sf, aes(colour = PRICE)) +\n  geom_sf() +\n  scale_colour_viridis_c(end = 0.9) +\n  theme_void() +\n  ggrepel::geom_text_repel(data = baltimore_label, aes(x = X, y = Y, label = STATION)) +\n  geom_point(data = baltimore_label, aes(x = X, y = Y), size = 3, shape = 1)\n\n\n\n\nIf we use the exponential function to model spatial correlation, do you expect the estimated correlation between random variables for price to be larger for locations 1 and 2 or for locations 2 and 3?\nWe can calculate the estimated correlation explicitly using the estimated range parameter and the exponential correlation formula in Equation 2.2. The estimated correlation for locations 1 and 2 is \\(e^{-42.72 / 10.6} = 0.0178\\) while the estimated correlation for locations 2 and 3 is \\(e^{-7.28 / 10.6} = 0.503\\). This tells us that, according to our model, the correlation between the random variables for price at locations 1 and 2 is negligible: knowing the realized house price at location 2 tells us very little about what the realized house price will be at location 1. On the other hand, the correlation between the random variables for price at locations 2 and 3 is moderately strong: knowing the realized house price at location 2 tells us that there is a strong possibility that the realized house price at location 3 is similar to the realized price at location 2.\nWe can also make a plot of the correlation as a function of distance. The maximum distance between houses in the data set is 128 units.\n\n\n\n\n\nFrom the correlation plot, we see that, as we would expect, as distance gets larger, the correlation decreases. We also see from the plot that the correlation approaches 0 at large distances but never reaches 0 exactly and never becomes negative.\nThere are many functions to choose from to model spatial correlation. We saw the formula for the exponential correlation function in Equation 2.2. Some other common spatial correlation functions include:\n\nthe spherical correlation function:\n\n\\[\n\\text{Corr}(Y_1, Y_2) =\n\\begin{cases}\n1 - 1.5 \\cdot \\frac{h_{12}}{\\phi} + 0.5 \\cdot \\left(\\frac{h_{12}}{\\phi}\\right)^3, & h_{12} \\leq \\phi \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\tag{2.3}\\]\n\nthe Gaussian correlation function:\n\n\\[\n\\text{Corr}(Y_1, Y_2) =\ne^{-\\left(\\frac{h_{12}}{\\phi}\\right)^2}.\n\\tag{2.4}\\]\n\nthe triangular correlation function:\n\n\\[\n\\text{Corr}(Y_1, Y_2) =\n\\begin{cases}\n\\left(1 - \\frac{h_{12}}{\\phi}\\right), & h_{12} \\leq \\phi \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\tag{2.5}\\]\nA plot of the estimated exponential, spherical, and Gaussian correlation functions for the housing data is shown here.\n\n\n\n\n\nNotice that all three correlation functions having decreasing correlation as distance increases and that all three are bound by \\(0\\) and \\(1\\). Note also that, while all three curves are somewhat similar, interpretating the nature of the spatial correlation in the house price data would change depending on which function is chosen. For example, for the Gaussian correlation, we would say that locations that are more than 12 or so units apart have no correlation in their prices. But, for the exponential and spherical correlation functions, we would still estimate there to be positive correlation at this distance. Which correlation function is best to use is a topic discussed more in Chapter 3.\n\n\n2.2.2 Introducing Spatial Covariance\nClosely tied to the concept of spatial correlation is the idea of spatial covariance. The spatial covariance between two random variables, \\(Y_1\\) and \\(Y_2\\), is simply equal to the numerator in Equation 2.1, or, rearranging Equation 2.1, is equal to:\n\\[\n\\text{cov}(Y_1, Y_2) = \\sigma_1 \\sigma_2 \\text{corr}(Y_1, Y_2).\n\\]\nIn many spatial models, we assume that the standard deviation (and therefore the variance) of \\(Y_1\\) and the standard deviation (and therefore the variance) of \\(Y_2\\) are equal (\\(\\sigma = \\sigma_1 = \\sigma_2\\), an assumption discussed more in Section 2.4) so that\n\\[\n\\text{cov}(Y_1, Y_2) = \\sigma_{de}^2 \\text{corr}(Y_1, Y_2),\n\\tag{2.6}\\]\nwhere \\(\\sigma_{de}^2\\) is often called the partial sill, or dependent error variance, parameter. We can use the same spatial models for \\(\\text{corr}(Y_1, Y_2)\\) that were described in the previous section. So, we really just have one extra parameter now: \\(\\sigma_{de}^2\\), which scales the correlation so that the maximum now corresponds to \\(\\sigma_{de}^2\\) instead of 1. For the housing data, if we were to estimate correlation of house prices in dollars and house prices in thousands of dollars with the same data, we would arrive at the same exact correlation function. However, the covariance would be larger if the prices were in dollars instead of in thousands of dollars.\nEstimating \\(\\sigma_{de}^2\\) (which is discussed more in Chapter 3) with an exponential correlation/covariance model gives a value of $_{de}^2 = $ 716.9. The estimated covariance between locations 1 and 2 is then \\(716.9 \\cdot e^{-42.72 / 10.6} = 12.76\\) while the estimated covariance between locations 2 and 3 is \\(716.9 \\cdot e^{-7.28 / 10.6} = 360.6\\).\nA plot of the estimated covariance of house prices, as a function of distance, is shown below. Note that the general pattern of the covariance is identical to that of the correlation; however, the y-axis is now scaled up to the estimated overall variance of house prices (just over 700 thousands of dollars squared).\n\n\n\n\n\nWhile the correlation between random variables is very commonly discussed in non-spatial random effects models and in time series models, the covariance is more commonly presented in spatial contexts.\nThough the covariance can be modeled using Equation 2.6, more commonly, we introduce a third covariance parameter to model spatial covariance. This third covariance parameter is often called the nugget and inflates the covariance only when the distance \\(h\\) is equal to 0. Including the nugget often helps with model fit, so, unless there is a good reason not to, most spatial models include a nugget parameter.\nThe exponential covariance between random variables \\(Y_i\\) and \\(Y_j\\) at spatial locations separated by a distance \\(h_{ij}\\), including a nugget effect, is:\n\\[\n\\text{cov}(Y_i, Y_j) =\n\\begin{cases}\n\\sigma_{ie}^2 + \\sigma_{de}^2 e^{\\frac{-h_{ij}}{\\phi}}, & h_{ij} = 0\\\\\n\\sigma_{de}^2 e^{\\frac{-h_{ij}}{\\phi}}, & h_{ij} &gt; 0\n\\end{cases}\n\\tag{2.7}\\]\nIn the formula, \\(\\sigma^2_{ie}\\) refers to the nugget, or independent error variance parameter while \\(\\sigma^2_{de}\\) refers to the partial sill, or the dependent error variance parameter.\n\n\n\n\n\n\nImportant\n\n\n\nThough we have built up to the spatial covariance function in Equation 2.7 slowly (by first introducing the range parameter, then the partial sill, and finally the nugget), it is Equation 2.7 with all three spatial covariance parameters that is most commonly used in spatial models (not Equation 2.2 or Equation 2.6).\n\n\nThe \\(\\text{cov}(Y_i, Y_j)\\), with a nugget effect can be similarly defined for the other correlation functions (spherical, gaussian, triangular) by subbing in the appropriate correlation function in for \\(e^{\\frac{-h_{ij}}{\\phi}}\\).\nAdding in a nugget parameter to the model for the covariance means that we can get different estimates for the partial sill and the range parameters. The plot below shows the fitted covariance using a gaussian correlation function both with a nugget effect and without a nugget effect for the housing price variable. Note in the plot below how the inclusion of a nugget effect results in a “jump” in covariance when the distance is equal to 0 while in the model without the nugget effect, no such “jump” is possible.\n\n\n\n\n\nIf there is no nugget effect in the model, then the correlation between \\(Y_i\\) and \\(Y_j\\) can be calculated as \\(e^{-\\left(\\frac{h_{ij}}{\\phi}\\right)^2}\\) for a gaussian correlation model. This is the formula we used when we introduced correlation at the beginning of the section.\nHowever, if there is a nugget effect, then the overall variance is inflated, so that the correlation between \\(Y_i\\) and \\(Y_j\\), whose locations are separated by distance \\(h_{ij}\\), is:\n\\[\n\\text{Corr}(Y_i, Y_j) = \\frac{\\sigma_{de}^2e^{-\\left(\\frac{h_{ij}}{\\phi}\\right)^2}}{\\sigma^2_{de} + \\sigma^2_{ie}}\n\\]"
  },
  {
    "objectID": "spatial-covariance.html#interpreting-spatial-covariance-parameters",
    "href": "spatial-covariance.html#interpreting-spatial-covariance-parameters",
    "title": "2  Spatial Covariance and Correlation",
    "section": "2.3 Interpreting Spatial Covariance Parameters",
    "text": "2.3 Interpreting Spatial Covariance Parameters\nLoosely speaking, the three covariance parameters can be interpreted as:\n\n\\(\\sigma^2_{ie}\\) is the amount of “independent” (non-spatial) error variability in the response.\n\\(\\sigma^2_{de}\\) is the amount of “dependent” (spatial) error variability in the response.\n\\(\\phi\\) controls the rate of decay in the spatial correlation so that larger values of \\(\\phi\\) indicate that the correlation decays more slowly with distance.\n\nHowever, in practice, simple interpretations of these three parameters on their own and without considering the units of distance in the data used to fit the model can lead to misguided statements. Consider, for example, the following sets of covariance parameters with an exponential correlation function for locations that have a minimum distance (non-zero) of 0.5 units and a maximum distance of 5 units:\n\nSet 1: \\(\\sigma^2_{ie} = 25\\), \\(\\sigma^2_{de} = 0.5\\), \\(\\phi = 2.5\\).\nSet 2: \\(\\sigma^2_{ie} = 1\\), \\(\\sigma^2_{de} = 25\\), \\(\\phi = 0.01\\).\n\nIt would be incorrect to conclude that, based on these parameters, there is a lot of spatial covariance in Set 2 (which has a very large proportion of the overall variance coming from \\(\\sigma^2_{de}\\)) while very little spatial covariance in Set 1 (which has a very large proportion of the overall variance coming from \\(\\sigma^2_{ie}\\). In fact, these models are very similar for the set of distances that are in our data.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe moral of this toy example is that plotting the spatial covariance function on a range of distances observed in the data is the best way to understand the nature of the spatial correlation in the variable of interest.\n\n\nNote that, for the same exact example, if our observed data was actually observed in a space such that the minimum distance between two locations is 0.001 units and the maximum distance between two locations is 0.01 units, the two models above are now quite different!\n\n\n\n\n\nSo, when interpreting spatial covariance parameters, we must keep the scale of distance in mind."
  },
  {
    "objectID": "spatial-covariance.html#sec-stationarity",
    "href": "spatial-covariance.html#sec-stationarity",
    "title": "2  Spatial Covariance and Correlation",
    "section": "2.4 Stationarity",
    "text": "2.4 Stationarity\nThe models that we have used for spatial covariance thus far all have an assumption of second-order stationarity. A process that satisfies a second-order stationarity assumption has:\n\nA constant mean (so, the mean does not shift from one location to another. In Chapter 3, we will allow the mean to change as a function of predictor variables, but we will make this stationarity assumption on the error terms in the model, which we will assume have a constant mean of 0).\nSpatial covariance that depends only on the distance and direction between two locations.\n\nSecond-order stationarity is a very common assumption in the field of spatial statistics. We will assume our data are generated from a second-order stationary process throughout most of these materials.\nAdditionally, the models that we used above are isotropic models in that the spatial covariance depends only on the distance between two spatial locations (and not the direction). Models that relax this assumption are called anisotropic models and allow the covariance for two locations separated by a distance \\(h_{ij}\\) in the north-south direction to be different than the covariance for two locations separated by the same distance \\(h_{ij}\\) in the east-west direction."
  },
  {
    "objectID": "point-modeling.html#motivation",
    "href": "point-modeling.html#motivation",
    "title": "3  Modeling Point Data",
    "section": "3.1 Motivation",
    "text": "3.1 Motivation\nThe caribou data set contains 30 observations from a caribou forage experiment in Alaska. The data, provided by Elizabeth Lenart of the Alaska Department of Fish and Game, is loaded with the spmodel package:\n\ncaribou\n## # A tibble: 30 × 5\n##   water tarp      z     x     y\n##   &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 Y     clear  2.42     1     6\n## 2 Y     shade  2.44     2     6\n## 3 Y     none   1.81     3     6\n## 4 N     clear  1.97     4     6\n## 5 N     shade  2.38     5     6\n## 6 Y     none   2.22     1     5\n## # ℹ 24 more rows\n\nThe caribou data frame contains the following variables, each collected at a point location in space:\n\nz, the percentage of nitrogen (the response variable for this example)\nx, the spatial x-coordinate\ny, the spatial y-coordinate\nwater, a factor with levels N for no water added and Y for water added\ntarp, a factor with levels clear for a clear tarp, none for no tarp, and shade for a shade tarp\n\nNote that caribou is not an sf object so there is no geometry column. Therefore, we will have to provide the x and y coordinates explicitly when we fit a spatial model or make a plot.\nOur analysis goal is to assess whether or not water was added and/or the type of tarp cover added are associated with the percentage of nitrogen measured at the location (z). Before we fit any models, we provide a few quick plots of our data:\n\nggplot(data = caribou, aes(x = x, y = y)) +\n  geom_point(aes(colour = z)) +\n  scale_colour_viridis_c()\n\n\n\n\nWe might also construct plots exploring the relationships between water and z as well as nitrogen and z:\n\nggplot(data = caribou, aes(x = water, y = z)) +\n  geom_boxplot()\n\n\n\nggplot(data = caribou, aes(x = tarp, y = z)) +\n  geom_boxplot() \n\n\n\n\nFrom these plots, we see that there is some evidence that tarp is associated with nitrogen, as observations with a shade tarp tend to have larger z values than observations with clear and none.\nWe will fit a spatial linear model with the spmodel package, exploring a few options and interpreting various parameter estimates."
  },
  {
    "objectID": "point-modeling.html#fitting-a-model",
    "href": "point-modeling.html#fitting-a-model",
    "title": "3  Modeling Point Data",
    "section": "3.2 Fitting a Model",
    "text": "3.2 Fitting a Model\nSuppose that we want to fit a model to this data set with z as the response variable and tarp and shade as predictors. We can fit such a model with standard linear regression (assuming independence), or, we can fit a spatial linear model that allows for locations closer in space to have random errors that are positively correlated.\nTo fit and summarise a model with independent random errors, we can use lm() and then obtain a tidy summary table of output with tidy():\n\nmod_lm &lt;- lm(z ~ water + tarp, data = caribou)\ntidy(mod_lm)\n## # A tibble: 4 × 5\n##   term        estimate std.error statistic  p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)   1.96      0.0696    28.2   5.27e-21\n## 2 waterY       -0.0352    0.0696    -0.506 6.17e- 1\n## 3 tarpnone      0.0497    0.0853     0.583 5.65e- 1\n## 4 tarpshade     0.251     0.0853     2.94  6.76e- 3\n\nThe output here should look familiar: we see coefficient estimates in the first column, standard errors in the second, and p-values for hypothesis tests that test whether each of the \\(\\beta\\) parameters in the model are different from 0 in the last column. This model assumes that the random error terms are all independent of one another.\nTo fit and summarise a model with spatially correlated random errors, we can use splm():\n\nmod_splm &lt;- splm(z ~ water + tarp, data = caribou,\n                 xcoord = x, ycoord = y, spcov_type = \"exponential\")\ntidy(mod_splm)\n## # A tibble: 4 × 5\n##   term        estimate std.error statistic  p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)   2.05      0.311       6.59 4.33e-11\n## 2 waterY       -0.0831    0.0645     -1.29 1.98e- 1\n## 3 tarpnone      0.0801    0.0776      1.03 3.02e- 1\n## 4 tarpshade     0.287     0.0767      3.74 1.86e- 4\n\nNote that there are three additional arguments to splm() that are not needed with lm(): the xcoord argument, the ycoord argument, and the spcov_type argument. The xcoord and ycoord arguments need to be specified because we need to tell splm() the columns that have the spatial coordinates (these can be left out if data is an sf object, which stores the coordinate information automatically in the geometry column). The spcov_type tells splm() which spatial covariance/correlation function we would like to use to model covariance on the errors.\nComparing the table of summary output for the independent error model and the spatial model, we see that most of the estimates, standard errors, and p-values are similar. (This is not always the case! Sometimes adding a spatial structure to the random errors yields very different estimates for the \\(\\beta\\) coefficients in the model). But, how can we interpret the results from the spatial model? How does the theoretical underpinning of the spatial model differ from that of the independent error model? And, which model is a better fit to the caribou data?"
  },
  {
    "objectID": "point-modeling.html#review-of-the-independent-error-model",
    "href": "point-modeling.html#review-of-the-independent-error-model",
    "title": "3  Modeling Point Data",
    "section": "3.3 Review of the Independent Error Model",
    "text": "3.3 Review of the Independent Error Model\nBefore discussing the theoretical underpinning of the spatial linear model, we first briefly review the linear model with independent random errors. Recall that a linear model with independent errors for the response z with tarp and water as predictors is\n\\[\nY_i = \\beta_0 + \\beta_1 tarpnone_i + \\beta_2 tarpshade_i + \\beta_3 water_i + \\epsilon_i,\n\\tag{3.1}\\]\nwhere \\(i\\) goes from \\(1, 2, \\ldots, n\\), \\(n\\) is the total number of observations in the sample, \\(Y_i\\) is the random variable for the percent nitrogen of the \\(i^{th}\\) observation. The terms \\(tarpnone_i\\), \\(tarpshade_i\\), and \\(water_i\\) are indicator variables that take on a 1 if observation \\(i\\) has no tarp, a 1 if observation \\(i\\) has a shaded tarp, and a 1 if observation \\(i\\) has water added, respectively. Importantly, for the independent error model, \\(\\epsilon_i\\), the random error for the \\(i^{th}\\) observation, has mean 0, constant variance \\(\\sigma_{ie}^2\\), and is independent of all other \\(\\epsilon_j\\). The independence of \\(\\epsilon_i\\) and \\(\\epsilon_j\\) implies that \\(\\text{Cov}(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i \\neq j\\).\nThe model for \\(Y_i\\) consists of two overarching components: a structure for the mean and a structure for the error.\nThe mean structure is the part of the model with all of the \\(\\beta_j\\) coefficients: \\(\\beta_0 + \\beta_1 tarpnone_i + \\beta_2 tarpshade_i + \\beta_3 water_i\\). This part of the structure allows \\(Y_i\\) to have a mean that depends on its predictor values for tarp and water. The error structure of the model is the part of the model with \\(\\epsilon_i\\). When we expand this model into a model that allows for spatial covariance, we will largely leave the mean structure untouched and will only modify the error structure to incorporate spatial covariance.\nA model with independent errors needs to estimate 5 total parameters: \\(\\beta_0\\), the intercept, \\(\\beta_1\\), \\(\\beta_2\\), and \\(\\beta_3\\), the slope parameters, and \\(\\sigma_{ie}^2\\), the independent error variance parameter.\nWe can again use the lm() function to fit and obtain a tidy summary of such a model with the caribou data with\n\nmod_lm &lt;- lm(z ~ water + tarp, data = caribou)\ntidy(mod_lm)\n## # A tibble: 4 × 5\n##   term        estimate std.error statistic  p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)   1.96      0.0696    28.2   5.27e-21\n## 2 waterY       -0.0352    0.0696    -0.506 6.17e- 1\n## 3 tarpnone      0.0497    0.0853     0.583 5.65e- 1\n## 4 tarpshade     0.251     0.0853     2.94  6.76e- 3\n\nThe five parameter estimates are \\(\\hat{\\beta}_0\\) = 1.96, \\(\\hat{\\beta}_1\\) = -0.035, \\(\\hat{\\beta}_2\\) = 0.05, \\(\\hat{\\beta}_3\\) = 0.251, and \\(\\hat{\\sigma}^2_{ie}\\) = 0.036.\nHowever, we might expect that, after accounting for the effects of tarp and water, there may be some spatial correlation in the random errors. That is, the assumption that \\(\\epsilon_i\\) is independent of all other \\(\\epsilon_j\\) may be unrealistic. We will compare this independent error model with a spatial model later on in Section 3.6."
  },
  {
    "objectID": "point-modeling.html#the-spatial-linear-model",
    "href": "point-modeling.html#the-spatial-linear-model",
    "title": "3  Modeling Point Data",
    "section": "3.4 The Spatial Linear Model",
    "text": "3.4 The Spatial Linear Model\nA spatial model extends the standard simple linear regression model with a correlated random error term:\n\\[\nY_i = \\beta_0 + \\beta_1 tarpnone_i + \\beta_2 tarpshade_i + \\beta_3 water_i + \\epsilon_i + \\tau_i,\n\\tag{3.2}\\]\nwhere \\(\\epsilon_i\\) is normally distributed with mean 0, variance \\(\\sigma^2_{ie}\\), and \\(\\epsilon_i\\) is independent of \\(\\epsilon_j\\) for all \\(i \\neq j\\). Note that Equation 4.1 is identical to Equation 3.1 except for the very last term \\(\\tau_i\\): we add this term in to slightly modify the error structure of the model while leaving the mean structure as is. This other random error term, \\(\\tau_i\\), is normally distributed with mean 0, variance \\(\\sigma^2_{de}\\), but can have covariance with \\(\\tau_j\\). So, while \\(\\epsilon_i\\) represents independent random error, \\(\\tau_i\\) represents spatailly dependent random error. We then model the covariance of \\(\\tau_i\\) and \\(\\tau_j\\) using a covariance model discussed in Chapter 2. For example, we might use the \"exponential\" covariance model to model the covariance between \\(\\tau_i\\) and \\(\\tau_j\\):\n\\[\\begin{equation}\n\\text{cov}(\\tau_i, \\tau_j) = \\sigma^2_{de} \\text{exp}(\\frac{-h_{ij}}{\\phi}),\n\\end{equation}\\]\nwhere \\(h_{ij}\\) is the Euclidian distance between the observations at locations \\(i\\) and \\(j\\), and \\(\\phi\\) is called the range parameter, which controls the rate of decay of the covariance as a function of distance.\nTherefore, with this model, if we assume that all \\(\\epsilon_i\\) are independent of all \\(\\tau_i\\), we have that:\n\\[\n\\text{cov}(Y_i, Y_j) = \\text{cov}(\\epsilon_i, \\epsilon_j) + \\text{cov}(\\tau_i, \\tau_j) =\n\\begin{cases}\n\\sigma_{ie}^2 + \\sigma_{de}^2 e^{\\frac{-h_{ij}}{\\phi}}, & h_{ij} = 0\\\\\n\\sigma_{de}^2 e^{\\frac{-h_{ij}}{\\phi}}, & h_{ij} &gt; 0\n\\end{cases}\n\\]\nThis is the same model that we used at the end of Chapter 2: we are now just using this model on the random errors of a linear regression model and allowing the \\(Y_i\\)’s to have different means that depend on the mean structure of the model.\nA spatial model with the exponential covariance function estimates 7 total parameters:\n\n\\(\\beta_0\\), the intercept, and \\(\\beta_1\\), \\(\\beta_2\\), and \\(\\beta_3\\), the slope parameters\n\\(\\sigma_{ie}^2\\), the independent variance parameter, \\(\\sigma^2_{de}\\), the spatially dependent variance parameter, and \\(\\phi\\), the range parameter.\n\nRecall that, in the spatial model, \\(\\sigma_{ie}^2\\) is sometimes referred to as the nugget, \\(\\sigma^2_{de}\\) is sometimes referred to as the partial sill, and \\(\\phi\\) is sometimes referred to as the range.\nWe re-fit a spatial linear model and obtain a tidy summary with\n\nmod_splm &lt;- splm(z ~ water + tarp, data = caribou,\n                 spcov_type = \"exponential\",\n                 xcoord = \"x\", ycoord = \"y\")\nmod_splm |&gt; tidy()\n## # A tibble: 4 × 5\n##   term        estimate std.error statistic  p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)   2.05      0.311       6.59 4.33e-11\n## 2 waterY       -0.0831    0.0645     -1.29 1.98e- 1\n## 3 tarpnone      0.0801    0.0776      1.03 3.02e- 1\n## 4 tarpshade     0.287     0.0767      3.74 1.86e- 4\n\nWhat changes about our interpretation of this summary output compared to the model with independent errors? Next, we will interpret some of the estimated model parameters and find and interpret confidence intervals for some of these parameters."
  },
  {
    "objectID": "point-modeling.html#interpreting-the-spatial-linear-model",
    "href": "point-modeling.html#interpreting-the-spatial-linear-model",
    "title": "3  Modeling Point Data",
    "section": "3.5 Interpreting the Spatial Linear Model",
    "text": "3.5 Interpreting the Spatial Linear Model\nBased on the previous output, our fitted spatial linear model is:\n\\[\n\\hat{Y}_i = 2.05 - 0.0831 water_i + 0.0801 tarpnone_i + 0.287 tarpshade_i\n\\]\nWe can find the estimated spatial covariance parameters with:\n\ntidy(mod_splm, effects = \"spcov\")\n## # A tibble: 3 × 3\n##   term  estimate is_known\n##   &lt;chr&gt;    &lt;dbl&gt; &lt;lgl&gt;   \n## 1 de      0.111  FALSE   \n## 2 ie      0.0226 FALSE   \n## 3 range  19.1    FALSE\n\nTherefore,\n\\[\n\\widehat{\\text{cov}}(Y_i, Y_j) =\n\\begin{cases}\n0.0226 + 0.111 e^{\\frac{-h_{ij}}{19.1}}, & h_{ij} = 0\\\\\n0.111 e^{\\frac{-h_{ij}}{19.1}}, & h_{ij} &gt; 0,\n\\end{cases}\n\\]\nwhere \\(h_{ij}\\) is the distance between locations \\(i\\) and \\(j\\). Note that, by default, these parameters are estimated using Restricted Maximum Likelihood (REML). The details of the theory behind REML parameter estimation is beyond the scope of these course materials, but we can think about these parameter estimates as providing the best “fit” to the observed data given the specification of the model.\nwater_i + tarpnone_i + tarpshade_i\nWe first focus on interpreting \\(\\hat{\\beta}_0 = 2.05\\), \\(\\hat{\\beta}_1 = -0.0831\\), \\(\\hat{\\beta}_2 = 0.0801\\), and \\(\\hat{\\beta}_3 = 0.287\\), the estimated regression coefficients in the spatial linear model. These can be interpreted in a similar way to how the fitted regression coefficients are interpreted in a multiple regression model with independent errors. For example, for the waterY coefficient interpretation of -0.0831, we might say something like:\nWe expect the average percentage of nitrogen for plots with water added to be 0.0831 percentage points lower than the average percentage of nitrogen for plots with no water added, for a fixed value of tarp.\nTo interpret the three spatial covariance parameters, we can make a covariance plot that has covariance on the y-axis and distance on the x-axis, just as we did in Chapter 2.\n\n\n\n\n\nWe see from the plot that the model estimates there to be some spatial covariance in the errors of the model for the entire range of distances that are present in the data. We also see that the estimated spatial covariance is slightly higher for locations that are closer together (the smallest non-zero distance in the data is 1 unit) than for locations that are further apart, so, a spatial model seems appropriate.\n\n3.5.1 Inference\nIn addition to interpreting the fitted model parameters, we can perform inference (confidence intervals and hypothesis tests) on the slope parameters.\nAs in a regression model with independent random errors, confidence intervals for the slope parameters (as well as the intercept) can be found with:\n\ntidy(mod_splm, conf.int = TRUE, conf.level = 0.95)\n## # A tibble: 4 × 7\n##   term        estimate std.error statistic  p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)   2.05      0.311       6.59 4.33e-11   1.44      2.66  \n## 2 tarpnone      0.0801    0.0776      1.03 3.02e- 1  -0.0720    0.232 \n## 3 tarpshade     0.287     0.0767      3.74 1.86e- 4   0.136     0.437 \n## 4 waterY       -0.0831    0.0645     -1.29 1.98e- 1  -0.209     0.0433\n\nAnd, these intervals can be interpreted in the same way. For example, to interpret the 95% confidence interval for \\(\\beta_2\\), the slope coefficient for tarpshade, we might say something like:\nWe are 95% confidence that the average nitrogen percentage for plots with a shade tarp cover is between 0.136 percentage points and 0.437 percentage points higher than the average nitrogen percentage for plots with a clear tarp cover, for a fixed value of water.\nWe can also make similar interpretations about p-values for hypothesis tests from the fitted model output. For example, the p-value in the tarpshade row tests the hypothesis:\n\\(H_0: \\beta_2 = 0\\) vs.\n\\(H_a: \\beta_2 \\neq 0\\)\nWith a small p-value of 0.00019, we have strong evidence that the average nitrogen percentage for plots with the shade tarp is different than the average nitrogen percentage for plots with the clear tarp (the reference group), after accounting for the effects of water.\nNote however, that the p-values in these hypothesis tests and the confidence intervals for the slope coefficients are asymptotic, meaning that they are only technically valid for very large sample sizes. They should be viewed as only approximate for small sample sizes. Because the caribou data only has 30 observations, the confidence intervals and p-values are only approximations and should be taken with a small grain of salt.\nAlso note that it is a common misconception that incorporating spatial dependence into the random errors of a model will always result in larger p-values. While this is true for some data sets, it is certainly not always the case. Sometimes, accounting for spatial correlation strengthens the “signal” from the predictors in the model, resulting in smaller p-values for tests of association between predictors and the response, as we see for each of the predictors in the caribou example."
  },
  {
    "objectID": "point-modeling.html#sec-compare",
    "href": "point-modeling.html#sec-compare",
    "title": "3  Modeling Point Data",
    "section": "3.6 Comparing Models",
    "text": "3.6 Comparing Models\nNow that we have discussed how to interpret the model parameters in a spatial linear model and how to conduct inference on some of these parameters, we will try to answer the question “is the spatial model a better fit to the data than the independent error model?” Or, put another way, is it necessary to fit a spatial model for this particular data set?\nWe will approach this question in two ways: we will make a visualization of the model residuals from the independent error model and we will compare the two models directly with a model fit criterion.\nFirst, if a model with independent random errors is sufficient, then we would expect there to be evidence of spatial correlation in the random errors of that model. But, we never actually observe realizations of the random errors. We do, however, observe residuals for a model. Recall that a residual for the \\(i^{th}\\) observation is defined as:\n\\[\ne_i = y_i - \\hat{y}_i,\n\\]\nwhere \\(y_i\\) is the observed response value for the \\(i^{th}\\) observation and \\(\\hat{y}_i\\) is the predicted response value for the \\(i^{th}\\) observation based on the predictor values associated with \\(y_i\\) and the estimated regression coefficients \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots\\). In the caribou example, for the independent error model,\n\\[\n\\hat{y}_i = 1.96 - 0.0352 water_i + 0.0497 tarpnone_i + 0.251 tarpshade_i.\n\\]\nIf the independent error model is sufficient, then, if we make a plot of the model residuals, we would expect to see very little evidence of spatial correlation.\n\ncaribou_r &lt;- caribou |&gt; mutate(.resid = residuals(mod_lm))\n\n\nggplot(data = caribou_r, aes(x = x, y = y,\n                             colour = .resid)) +\n  geom_point() +\n  scale_colour_viridis_c(limits = c(-0.5, 0.5))\n\n\n\n\nFrom this plot, we “kind of” see evidence of spatial correlation in the residuals: the points in the upper-left do seem like they have generally higher residuals than the points in the bottom-right. But, with only 30 data points, making this subjective assessment is extremely tough.\nAn alternative approach is to compare the models more formally using model fit criteria like AIC or AICc. We do not define these explicitly here, but recall that lower AIC and AICc values are generally better. Intuitively, both of these criteria provide a balance between model fit and the number of parameters needed to fit the model. So, if two models provide roughly equal “fit,” the model with fewer parameters will have the lower (and better) AIC and AICc values.\nWe can also use these model fit criteria to assess the fit of a couple of other covariance functions, like the \"gaussian\" and the \"spherical\".\n\nmany_mods &lt;- splm(z ~ water + tarp, data = caribou,\n                 xcoord = x, ycoord = y,\n                 spcov_type = c(\"none\", \"exponential\", \"spherical\", \"gaussian\"))\n\nglances(many_mods)\n## # A tibble: 4 × 11\n##   model           n     p  npar value   AIC  AICc   BIC logLik deviance\n##   &lt;chr&gt;       &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n## 1 none           30     4     1 -1.68 0.323 0.466  1.72  0.838     26  \n## 2 exponential    30     4     3 -5.85 0.150 1.07   4.35  2.92      26.0\n## 3 spherical      30     4     3 -5.84 0.155 1.08   4.36  2.92      26.0\n## 4 gaussian       30     4     3 -5.69 0.314 1.24   4.52  2.84      26.0\n## # ℹ 1 more variable: pseudo.r.squared &lt;dbl&gt;\n\nIn the code above, spcov_type is a vector of possible covariance functions. Each different covariance function is fit as a model for the errors. Setting spcov_type to \"none\" tells splm() to not use any spatial covariance and to just fit a regression model with independent random errors. we then use glances() to compare AIC for the fitted models. We see that, while the exponential model has a slightly lower AIC than the others, an independent error model fits almost as well and actually has a lower AICc value.\nIn this section, we have fit and interpreted a formal spatial model using a data set on caribou as our example. In the next section, we turn our attention to the assumptions we need to make to fit such a model."
  },
  {
    "objectID": "model-assumptions.html#model-assumptions",
    "href": "model-assumptions.html#model-assumptions",
    "title": "4  Model Assumptions",
    "section": "4.1 Model Assumptions",
    "text": "4.1 Model Assumptions\nFor spatial linear models, we must make some assumptions about the underlying model generating our data. What are these assumptions, and how can we check these assumptions given our observed sample of data? All of the assumptions are embedded into the spatial linear model formula:\n\\[\nY_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_p x_{pi} + \\epsilon_i + \\tau_i,\n\\tag{4.1}\\]\nwhere\n\n\\(\\epsilon_i\\) is normally distributed with mean 0, variance \\(\\sigma^2_{ie}\\), and \\(\\epsilon_i\\) is independent of \\(\\epsilon_j\\) for all \\(i \\neq j\\).\n\\(\\tau_i\\) is normally distributed with mean 0, variance \\(\\sigma^2_{de}\\), and can have covariance with \\(\\tau_j\\). The covariance of \\(\\tau_i\\) and \\(\\tau_j\\) can be modeled with a covariance function, like the exponential, gaussian, etc, that depends only on the distance \\(h_{ij}\\) between observations \\(i\\) and \\(j\\).\n\nHere, we will also define \\(\\delta_i \\equiv \\epsilon_i + \\tau_i\\) to encompass the entire random error. Because \\(\\epsilon_i\\) and \\(\\tau_i\\) are independent, the mean of \\(\\delta_i\\) is equal to 0, the variance of \\(\\delta_i\\) is equal to \\(\\sigma^2 \\equiv \\sigma_{ie}^2 + \\sigma_{de}^2\\), and the covariance of \\(\\delta_i\\) with \\(\\delta_j\\) (\\(i \\neq j\\)) with an exponential correlation function is equal to \\(\\sigma_{de}^2 e^{-h_{ij} / \\phi}\\), where \\(h_{ij}\\) is the distance between locations \\(i\\) and \\(j\\) and \\(\\phi\\) is the range parameter.\nEmbedded within this spatial linear model formula are the assumptions of:\n\nLinearity\nStationarity and Isotropy\nNormality\n\nWe examine each of these assumptions in turn here.\n\n4.1.1 Linearity\nFirst, like in regression models with independent random errors, we still assume that the “mean structure” of the model is linear with respect to the \\(\\beta\\) coefficients. The most common structure for the mean of the response \\(Y_i\\), which we will denote as \\(\\mu_i\\), is: \\(\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_p x_{pi}\\).\nHowever, spatial linear models can still easily accommodate non-linear trends as long as the mean structure is linear in the \\(\\beta\\) coefficients. For example,\n\n\\(\\mu_i = \\beta_0 + \\beta_1 x_{1i}^2\\),\n\\(\\mu_i = \\beta_0 + \\beta_1 log(x_{1i}) + \\beta_2 \\sqrt{x_{2i}}\\),\n\\(\\mu_i = \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{1i} x_{2i}\\)\n\nare all linear models in that they are linear in the \\(\\beta_i\\) coefficients. An example of a non-linear mean structure for a model is \\(\\mu_i = \\beta_0 + \\beta_1e^{-x_{1i}/\\beta_2}\\) because this cannot be re-written to be linear in the \\(\\beta_i\\) coefficients. Additive models and splines are other examples of methods that can be used within the spatial linear model framework.\nThe most common plot to assess whether the linearity assumption is valid for a particular model is to plot the standardized residuals vs. the fitted values and look to make sure there is no strong curvature in the plot. The fitted values are defined as \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_{i1} + \\hat{\\beta}_2x_{i2} + \\ldots + \\hat{\\beta}_p x_{ip}\\). While the “non-standardized” residuals are \\(e_i = y_i - \\hat{y}_i\\), the standardized residuals are scaled so that they have a mean of 0 and a variance equal to 1.\nAs an example, we fit a spatial linear model with the spherical covariance function and with NROOM, NBATH, SQFT, LOTSZ, and AGE as predictors in the model.\n\nmod_lin &lt;- splm(PRICE ~ NROOM + NBATH + SQFT + LOTSZ,\n                data = baltimore_sf, spcov_type = \"spherical\")\n\nTo obtain the standardized residuals and the fitted values for the model, we augment() mod_lin.\n\nmod_lin_aug &lt;- augment(mod_lin)\nmod_lin_aug\n## Simple feature collection with 211 features and 10 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: 860 ymin: 505.5 xmax: 987.5 ymax: 581\n## CRS:           NA\n## # A tibble: 211 × 11\n##   PRICE NROOM NBATH  SQFT LOTSZ .fitted .resid    .hat  .cooksd .std.resid\n## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n## 1  47       4   1    11.2   5.7    48.3  -1.31 0.00926 0.0109        2.41 \n## 2 113       7   2.5  28.9 280.     91.2  21.8  0.0384  0.0127        1.26 \n## 3 165       7   2.5  30.6  70.6    74.2  90.8  0.0375  0.255         5.72 \n## 4 104.      7   2.5  26.1 175.     82.3  22.0  0.0156  0.000652      0.454\n## 5  62.5     7   1.5  22.0 108.     68.2  -5.70 0.0205  0.00721      -1.31 \n## 6  70       6   2.5  39.4 140.     78.5  -8.46 0.0476  0.0301       -1.73 \n## # ℹ 205 more rows\n## # ℹ 1 more variable: geometry &lt;POINT&gt;\n\nThe fitted values are given in a column called .fitted while the standardized residuals are given in a column called .std.resid. Then, we can construct a plot of the standardized residuals vs. fitted values with:\n\nggplot(data = mod_lin_aug, aes(x = .fitted, y = .std.resid)) +\n  geom_point()\n\n\n\n\nThere is not any major curvature in this plot: therefore, there is no major evidence that the linearity assumption has been violated for this model.\n\n\n4.1.2 Stationarity\nNext, we typically assume a type of stationarity in our model. If the stationarity assumption is valid, then we would expect the variance of \\(\\delta_i\\) to be similar at all values of the predictor variables. That is, we assume that all \\(\\delta_i\\) have the same overall variance: \\(\\sigma^2 \\equiv \\sigma_{de}^2 + \\sigma_{ie}^2\\).\nWe also assume that the covariance between \\(\\delta_i\\) and \\(\\delta_j\\) is a function of the distance between the locations \\(i\\) and \\(j\\) only. We can relax this assumption and assume that the covariance is a function of both distance and direction between the locations \\(i\\) and \\(j\\) by fitting an anisotropic model. However, in this section, we only consider isotropic models for which the covariance is only a function of distance.\nTo assess whether it is reasonable to assume that the random errors all have the same variance \\(\\sigma^2\\), we can again use the plot of the standardized residuals vs. the fitted values.\n\nggplot(data = mod_lin_aug, aes(x = .fitted, y = .std.resid)) +\n  geom_point()\n\n\n\n\nFrom this plot, we see that there is some mild evidence against this assumption. For predicted \\(\\hat{y}_i\\) that are small, there is less overall variability in the standardized residuals. On the other hand, for large fitted values, there is a bit more overall variability in the standardized residuals. However, this is not a drastic pattern: some may look at this plot and say that the assumption of constant variance seems reasonable while others might not be satisfied with that particular assumption.\nAssessing whether or not it is reasonable to assume that spatial covariance is based only on distance and that the chosen covariance function is reasonable is a much harder task visually. With larger data sets, we could split the data up into 4 “quadrants” and fit a spatial model separately for each quadrant. If the covariance structure does not change across space, then we would expect to get similar covariance structures in each of the 4 quadrants. But, with many examples, such as this one, we do not have enough data (211 observations here) to perform this check.\n\n\n\n\n\n\nImportant\n\n\n\nImportantly, the assumptions we have are much easier to think about in terms of the random errors, not in terms of the response variable itself.\n\n\nWhen we introduced spatial covariance in Chapter 2, we did so in terms of a response variable of interest. However, after we introduce predictor variables into the model, we are now thinking about this covariance as being a structure placed on the random errors in the model. We might wind up with really different fitted covariance functions for a model placed on the response variable (with no predictors) than for a model placed on the random errors with predictors included in the mean structure of the model.\nFor example, consider again the baltimore housing data set. We can plot the response variable, PRICE and observe that there is substantial spatial correlation in the PRICE variable:\n\nggplot(data = baltimore_sf, aes(colour = PRICE)) +\n  geom_sf() +\n  scale_colour_viridis_c() +\n  theme_void()\n\n\n\n\nAnd, a fitted covariance model for PRICE using a spherical covariance function with no predictors in the mean structure of the model is:\n\n\n\n\n\nWe see that there is a large amount of spatial covariance estimated. However, when we introduce predictors into our model, we model the random errors (\\(\\delta_i\\)) with the chosen spatial covariance function. Doing so can drastically change the estimated covariance parameters:\n\n\n\n\n\nIn the plot above, we see that, when modeling PRICE with no predictors, there is more overall variability (a larger covariance when distance is equal to 0) and that, in general, the covariance between PRICE random variables is larger. On the other hand, when we introduce predictors into the model, some of the variability in PRICE is accounted for (or explained) by these predictors. Therefore, we see that the fitted covariance curve is generally lower at all values of distance compared to the curve for the model with no predictors.\n\n\n\n\n4.1.3 Normality\nFinally, we also assume that the random errors in the model are normally distributed.\n\n\n\n\n\n\nImportant\n\n\n\nThe assumption here is that the errors are normally distributed. Especially if the distribution of one or more of the predictors is very skewed, then it is entirely possible for a histogram of the response to look very skewed but for a histogram of the residuals to look approximately normally distributed.\n\n\nTo assess this assumption, we can construct a histogram of the standardized residuals.\n\nggplot(data = mod_lin_aug, aes(x = .std.resid)) +\n  geom_histogram(colour = \"black\", fill = \"white\", bins = 15)\n\n\n\n\nWe see that the histogram of residuals is approximately symmetric and that there is one fairly extreme outlier with a standardized residual equal to 6.\nNote that if we construct a histogram of the response variable, PRICE, we observe some right-skewness.\n\nggplot(data = baltimore_sf, aes(x = PRICE)) +\n  geom_histogram(colour = \"black\", fill = \"white\", bins = 15)\n\n\n\n\nThe reason that we can see skewness in the response variable but not in the residuals is that, for the histogram of the residuals, we are taking into account the effects of the predictors. A simple linear regression example can help illustrate this. In the toy example below, we see that the observed distribution of the toy response variable, \\(y\\), is clearly right-skewed.\n\nset.seed(5142141)\nx &lt;- rexp(100, 1)\ny &lt;- 2 + 4 * x + rnorm(100, 0, 1)\ntoy_df &lt;- tibble(x, y)\nggplot(data = toy_df, aes(x = y)) +\n  geom_histogram(colour = \"black\", fill = \"white\", bins = 15)\n\n\n\n\nThe reason for this right-skewness is that there are many small \\(x\\) values but not very many observed large \\(x\\) values:\n\nggplot(data = toy_df, aes(x = x, y = y)) +\n  geom_point()\n\n\n\n\nAfter we account for \\(x\\) in the model, we obtain a symmetric distribution of the standardized model residuals, satisfying the normality assumption even though the observed histogram of \\(y\\) is right-skewed.\n\ntoy_mod &lt;- lm(y ~ x, data = toy_df)\ntoy_resid &lt;- augment(toy_mod)\nggplot(data = toy_resid, aes(x = .std.resid)) +\n  geom_histogram(colour = \"black\", fill = \"white\", bins = 15)\n\n\n\n\nAgain, we only care about normality of the random errors, so as long as the histogram of the residuals is not very skewed, the normality assumption is satisfied.\nNormality of the random errors is most important for constructing prediction intervals for the response at unsampled spatial locations. For other types of inference, including hypothesis tests and confidence intervals on the \\(\\beta\\) coefficients and confidence intervals for the mean response at a sampled or unsampled spatial location, mild violations of normality are okay for large sample sizes because of the Central Limit Theorem.\n\n\n4.1.4 Randomness\nIn the discussion of assumptions above, a “random selection” of spatial locations is not mentioned. Random selection of spatial locations is not a strict assumption for fitting and interpreting a spatial model. Often, randomly selecting spatial locations to sample in a study area is not feasible anyway, as sampling on private land or on certain terrain may be illegal or impossible.\nWhile randomness is not a strict assumption for a spatial model, how spatial locations are selected can influence the scope of inference. As an extreme example, we would not want to measure pollutant concentration in a sample of lakes in Minnesota and make subsequent inference to all lakes in the United States. As a less extreme example, we would not want to measure pollutant concentration in only the largest lakes in Minnesota and make subsequent inference to all lakes in Minnesota."
  },
  {
    "objectID": "model-assumptions.html#addressing-violated-assumptions",
    "href": "model-assumptions.html#addressing-violated-assumptions",
    "title": "4  Model Assumptions",
    "section": "4.2 Addressing Violated Assumptions",
    "text": "4.2 Addressing Violated Assumptions\nWe now turn our attention to addressing violated assumptions. Most of the strategies discussed here substantially overlap with strategies used to address violated assumptions for linear models with independent random errors.\n\n4.2.1 Adjusting the Mean Structure\nSometimes the assumptions of a linear model are violated because the mean structure of the model is either missing an important predictor or is mis-specified in some other way. To remedy this issue, we can add in the important predictor to the mean structure of the model, or we can transform a predictor that is already in the model (e.g., with the log transformation, square root transformation, etc.).\nWe have actually already seen this in the toy example given for the normality assumption. With no predictors in the model, we saw that normality was violated. However, after we introduced the predictor \\(x\\) into the mean structure of the model, the residuals no longer appeared skewed.\nAs another example, suppose that we fit a spatial linear model to the baltimore housing data set with PRICE as the response and SQFT as the only predictor. Making a plot of the standardized residuals vs. the fitted values, we see that there is a strong violation of the “constant variance” part of the stationarity assumption.\n\nmod_one &lt;- splm(PRICE ~ SQFT,\n                data = baltimore_sf, spcov_type = \"spherical\")\nmod_one_aug &lt;- augment(mod_one)\nmod_one_aug\n## Simple feature collection with 211 features and 7 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: 860 ymin: 505.5 xmax: 987.5 ymax: 581\n## CRS:           NA\n## # A tibble: 211 × 8\n##   PRICE  SQFT .fitted .resid    .hat .cooksd .std.resid\n## * &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n## 1  47    11.2    73.1  -26.1 0.00692 0.0131       1.94 \n## 2 113    28.9    88.0   25.0 0.00898 0.0209       2.15 \n## 3 165    30.6    89.4   75.6 0.0141  0.146        4.51 \n## 4 104.   26.1    85.6   18.7 0.00742 0.00206      0.743\n## 5  62.5  22.0    82.2  -19.7 0.00407 0.00465     -1.51 \n## 6  70    39.4    96.8  -26.8 0.0214  0.0531      -2.20 \n## # ℹ 205 more rows\n## # ℹ 1 more variable: geometry &lt;POINT&gt;\n\nggplot(data = mod_one_aug, aes(x = .fitted, y = .std.resid)) +\n  geom_point()\n\n\n\n\nAdding back in the other predictors (NROOM, NBATH, LOTSZ) and fitting the model with all four of these predictors gives a plot that shows a more mild violation of the constant variance assumption:\n\nggplot(data = mod_lin_aug, aes(x = .fitted, y = .std.resid)) +\n  geom_point()\n\n\n\n\n\n\n4.2.2 Transforming the Response\nIn the previous example, adding in the other predictors to the mean structure of the model helps the constant variance assumption. But, the plot of the standardized residuals vs. the fitted values still shows a mild departure from this assumption. If we think that this is cause for concern, we can transform the response variable. From such a transformation, we typically gain confidence in the model assumptions on the transformed response compared to the untransformed response. However, we typically lose interpretability of the model, as interpreting the estimated \\(\\hat{\\beta}_j\\) coefficients with a transformed response is much more challenging.\nAs an example, we can use the square root function to transform the PRICE variable in the spatial linear model with all 4 predictors.\n\nmod_trans &lt;- splm(sqrt(PRICE) ~ NROOM + NBATH + SQFT + LOTSZ,\n                data = baltimore_sf, spcov_type = \"spherical\")\nmod_trans_aug &lt;- augment(mod_trans)\n\nggplot(data = mod_trans_aug, aes(x = .fitted, y = .std.resid)) +\n  geom_point()\n\n\n\n\nWe see that the constant variance part of the stationarity assumption is more reasonable when modeling the square root of PRICE instead of PRICE directly.\n\n\n4.2.3 Adjusting the Error Structure\nThe strategies above (adjusting the mean structure and transforming the response) are applied to remedying model assumptions in many contexts outside of spatial statistics. We can also consider modifying the structure for the random errors in the model.\nThe most common method of adjusting the error structure in spatial models is to change the correlation function used. Thus far, we have only mentioned the exponential, spherical, gaussian, and triangular correlation functions (all of which use 3 parameters), but, there are many others. One of the more popular functions that we have not mentioned is the Matern correlation function, which uses 4 parameters: the partial sill, nugget, range, and an “extra” parameter that controls the “smoothness” of the correlation function.\nChanging the correlation function used to model the random errors may help with the stationarity assumption and the normality assumption of these errors. So, which correlation function is “correct” to use in a particular situation? That question can be quite challenging to answer. For many data sets, many of the commonly used spatial covariance functions will yield similar inferences on the fixed effect (\\(\\beta_j\\)) coefficients.\nPerhaps the most common method to choose a spatial covariance function is to use a model selection criterion like the AIC. For example, in the code below, we fit models with the exponential, gaussian, spherical, matern, and none covariance structures.\n\nmany_mods &lt;- splm(PRICE ~ NROOM + NBATH + SQFT + LOTSZ,\n                data = baltimore_sf,\n                spcov_type = c(\"exponential\", \"gaussian\", \"spherical\",\n                               \"matern\", \"none\"))\nglances(many_mods)\n## # A tibble: 5 × 11\n##   model           n     p  npar value   AIC  AICc   BIC logLik deviance\n##   &lt;chr&gt;       &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n## 1 spherical     211     5     3 1715. 1721. 1721. 1731.  -857.     206.\n## 2 exponential   211     5     3 1715. 1721. 1721. 1731.  -857.     206.\n## 3 gaussian      211     5     3 1722. 1728. 1729. 1739.  -861.     206.\n## 4 matern        211     5     4 1721. 1729. 1729. 1742.  -860.     206.\n## 5 none          211     5     1 1794. 1796. 1796. 1799.  -897.     206 \n## # ℹ 1 more variable: pseudo.r.squared &lt;dbl&gt;\n\nWe observe something common in spatial analysis here: choosing to use a spatial covariance function in the first place is much more important than which particular covariance function is chosen. In other words, we see that the model with none covariance has a much higher AIC than the other models: this model clearly provides the worst fit to the data of the 5 chocies. Then, while the spherical and exponential slightly outperform the gaussian and matern for this example, all four remaining models have similar AIC values. Additionally, if we examine the model summary output for some of the models with\n\nmany_mods[[3]] |&gt; tidy() ## spherical\n## # A tibble: 5 × 5\n##   term        estimate std.error statistic     p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n## 1 (Intercept)  30.0      14.3        2.09  0.0363     \n## 2 NROOM         2.15      1.09       1.97  0.0489     \n## 3 NBATH         8.27      1.86       4.44  0.00000880 \n## 4 SQFT          0.0870    0.171      0.508 0.612      \n## 5 LOTSZ         0.0821    0.0158     5.18  0.000000221\nmany_mods[[4]] |&gt; tidy() ## matern\n## # A tibble: 5 × 5\n##   term        estimate std.error statistic     p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n## 1 (Intercept)  19.1       7.30       2.61  0.00894    \n## 2 NROOM         2.12      1.11       1.92  0.0554     \n## 3 NBATH         8.39      1.89       4.44  0.00000904 \n## 4 SQFT          0.0969    0.172      0.562 0.574      \n## 5 LOTSZ         0.0843    0.0159     5.29  0.000000119\n\nwe see that the estimated coefficients and p-values for inference are quite similar in all of the models with a spatial covariance function.\nOther modifications that we can make to the error structure of the model include:\n\nadding anisotropy so that covariance depends on both direction and distance.\nadding non-spatial random effects with random to accommodate for things like repeated measurements at the same spatial location.\nadding in a partition factor with partition_factor to force observations in different levels of the partition factor to be uncorrelated, no matter how far apart they are in space.\n\nWhile all of these are possible to fit with spmodel (via the anisotropy, random, and partition_factor arguments to splm()), we do not explore these further in these materials. Instead, https://usepa.github.io/spmodel/articles/guide.html gives some examples on how to incorporate these more advanced modeling features into a spatial model."
  },
  {
    "objectID": "prediction.html#data-re-introduction",
    "href": "prediction.html#data-re-introduction",
    "title": "5  Prediction",
    "section": "5.1 Data (Re)-Introduction",
    "text": "5.1 Data (Re)-Introduction\nNow that we have some experience with more formal spatial modeling, we will go back and use sulfate data again to fit a formal spatial linear model. Recall that we had constructed a plot of the response variable, sulfate concentration, at the measured spatial locations (note that to recreate the plot, the ggspatial package is needed to reconstruct the scale bar):\n\nggplot(data = sulfate, aes(colour = sulfate)) +\n  geom_sf() +\n  scale_colour_viridis_c() +\n  theme_void() +\n  labs(colour = \"Sulfate (kg per ha)\") +\n  ggspatial::annotation_scale()\n\n\n\n\nIn the plot, we observe some spatial correlation in the sulfate values. There are no candidate predictors to use, so we would expect to see a lot of evidence of spatial correlation in a fitted model. In the model below, we specify a \"gaussian\" covariance function with splm():\n\nmod_sulf &lt;- splm(sulfate ~ 1, data = sulfate, spcov_type = \"gaussian\")\ntidy(mod_sulf)\n## # A tibble: 1 × 5\n##   term        estimate std.error statistic p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1 (Intercept)     8.22      2.54      3.24 0.00120\ntidy(mod_sulf, effects = \"spcov\")\n## # A tibble: 3 × 3\n##   term  estimate is_known\n##   &lt;chr&gt;    &lt;dbl&gt; &lt;lgl&gt;   \n## 1 de        48.3 FALSE   \n## 2 ie        13.7 FALSE   \n## 3 range 875795.  FALSE\n\nTherefore,\n\\[\n\\widehat{\\text{cov}}(Y_i, Y_j) =\n\\begin{cases}\n13.7 + 48.3 e^{-\\left(\\frac{h_{ij}}{875795}\\right)^2}, & h_{ij} = 0\\\\\n48.3 e^{-\\left(\\frac{h_{ij}}{875795}\\right)^2}, & h_{ij} &gt; 0\n\\end{cases},\n\\]\nwhere \\(Y_i\\) is the random variable for sulfate concentration at location \\(i\\), \\(Y_j\\) is the random variable for sulfate concentration at location \\(j\\), and \\(h_{ij}\\) is the distance between locations \\(i\\) and \\(j\\). And, we can see from the following plot that there is a large amount of covariance between sulfate concentration at spatial locations that are close together."
  },
  {
    "objectID": "prediction.html#prediction",
    "href": "prediction.html#prediction",
    "title": "5  Prediction",
    "section": "5.2 Prediction",
    "text": "5.2 Prediction\nWe should be able to use the fact that sulfate concentrations are spatially correlated to produce a more precise prediction for sulfate at an unsampled location than if we ignored the spatial nature of the data entirely.\nFor example, suppose that the sulfate value in the seventh row of the sulfate data set was actually missing:\n\nsulfate_na &lt;- sulfate |&gt; mutate(sulfate = if_else(row_number() == 7,\n                                                  true = NA,\n                                                  false = sulfate))\n\nWe want to predict the sulfate concentration at this spatial location:\n\nggplot(data = sulfate_na, aes(colour = sulfate)) +\n  geom_sf() +\n  scale_colour_viridis_c() +\n  theme_void() +\n  labs(colour = \"Sulfate (kg per ha)\") +\n  geom_sf(data = sulfate_na |&gt; slice(7), shape = 21, size = 3, colour = \"black\") +\n  geom_sf(data = sulfate_na |&gt; slice(6, 8), shape = 0, size = 3, colour = \"black\") +\n  labs(caption = \"The unsampled location has a black circle around it \\n while locations 6 (middle of plot) and 8 (bottom left of plot), \\n referenced more below, have black squares around them.\")\n\n\n\n\nA naive prediction of the sulfate concentration that completely ignores space would be the sample mean:\n\nsulfate_na |&gt;\n  summarise(naive_pred = mean(sulfate, na.rm = TRUE)) |&gt;\n  pull(naive_pred)\n## [1] 12.23997\n\nHowever, we can clearly see from the plot that the location is near a lot of other points with low sulfate concentration. We should be able to use this “extra” spatial information to obtain a more precise prediction for sulfate concentration.\nPrediction in the spatial context is often called “kriging.” To obtain the sulfate prediction at the unsampled location, we first re-fit the spatial model without the sulfate value at the seventh spatial location.\n\nmod_sulf_na &lt;- splm(sulfate ~ 1, data = sulfate_na, spcov_type = \"gaussian\")\ntidy(mod_sulf_na)\n## # A tibble: 1 × 5\n##   term        estimate std.error statistic p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1 (Intercept)     8.21      2.54      3.23 0.00126\n\nTo obtain the predicted sulfate concentration at the unobserved spatial location, we can use the predict() function:\n\npredict(mod_sulf_na)\n##        7 \n## 2.985773\n\nThis returns the predicted sulfate concentration for any row in the data set that had a missing NA value for sulfate. We obtain a prediction of about 3 kg per ha, which makes some sense when looking at the plot of the data. But, where does this value come from exactly? While the kriging equations are fairly complex for the scope of these materials (which are meant to be introductory), we can gain a little bit of intuition with how spatial information is incorporated into the prediction. With an estimated intercept value of \\(\\hat{\\beta}_0 = 8.21\\), the predicted sulfate concentration at the seventh spatial location can be calculated as:\n\\[\n\\dot{y}_7 = 8.21 + [w_1 (y_1 - 8.21) + \\ldots + w_6(y_6 - 8.21) + w_8(y_8 - 8.21) + \\ldots w_{197}(y_{197} - 8.21)],\n\\]\nwhere \\(\\dot{y}_7\\) is the predicted sulfate at location \\(7\\) and the \\(w_i\\)’s are weights applied to the sulfate values found at each of the observed spatial locations minus the overall mean. In general, a \\(w_i\\) is larger if the location \\(i\\) is closer to location \\(7\\), implying that the spatial covariance between the two locations is fairly high. Examining the sulfate data again, we would expect \\(w_1, w_2, \\ldots w_6\\) to all be relatively small, as these spatial locations are far from location \\(7\\) while \\(w_8, w_9, w_{10}\\) should all be relatively large, as these spatial locations are much closer to location \\(7\\). The larger a \\(w_i\\), the more “weight” observation \\(i\\) has in determining how much above or below the mean (of 8.21) our prediction should be. For example, because location \\(8\\) is much closer to location \\(7\\) than location \\(6\\) is, \\(w_8\\) is \\(0.087\\) while \\(w_6\\) is only \\(0.00456\\). So, the eighth location adjusts the prediction down by \\(0.087 (0.425 - 8.21) = -0.677\\) kg per ha while the sixth location adjusts the prediction up by only \\(0.00456 (15.358 - 8.21) = 0.033\\) kg per ha.\nWhile it is not possible to introduce the equations that form the \\(w_i\\) without introducing matrix notation for the model, any interested reader with a little background on matrices can examine https://usepa.github.io/spmodel/articles/technical.html#sec:predict-lm for the kriging equations."
  },
  {
    "objectID": "prediction.html#prediction-intervals",
    "href": "prediction.html#prediction-intervals",
    "title": "5  Prediction",
    "section": "5.3 Prediction Intervals",
    "text": "5.3 Prediction Intervals\nIn addition to obtaining a predicted sulfate value at an unobserved spatial location, we can also obtain a confidence interval for the mean sulfate at this location and a prediction interval for a range of reasonable sulfate values at this spatial location. Both of these intervals have very similar interpretations to how they are interpreted in a regression model with independent errors.\nFirst, we obtain a 95% confidence interval for the mean sulfate with:\n\npredict(mod_sulf_na, interval = \"confidence\", level = 0.95)\n##        fit      lwr      upr\n## 7 8.205691 3.220043 13.19134\n\nNote that the interval is centered around the estimated mean of 8.21, not around the prediction of 2.99. So, we are 95% confident that the mean sulfate concentration is between 3.22 kg per ha and 13.19 kg per ha.\nWe are usually more interested in a prediction interval. We can obtain a 95% prediction interval for a plausible range of sulfate values at the unobserved spatial location with:\n\npredict(mod_sulf_na, interval = \"prediction\", level = 0.95)\n##        fit       lwr      upr\n## 7 2.985773 -4.961117 10.93266\n\nWe are 95% confident that the sulfate concentration at that particular spatial location is actually between -4.96 kg per ha and 10.93 kg per ha. In this example, we have a negative lower bound, which can occur when we use the normal distribution, having support from \\(-\\infty\\) to \\(\\infty\\) as our model for the random errors. We should, however, check to see if there is any violation in the normality assumption: recall from Chapter 4 that a mild violation in normality is okay for most types of inference if we have a fairly large sample size, but prediction intervals are heavily reliant on the normality assumption.\n\naug_sulf_na &lt;- augment(mod_sulf_na)\nggplot(data = aug_sulf_na, aes(x = .std.resid)) +\n  geom_histogram(colour = \"black\", fill = \"white\", bins = 15)\n\n\n\n\nWe see that the normality assumption seems reasonable, as the standardized residuals appear approximately normally distributed, so interpreting a prediction interval should be okay in this context."
  },
  {
    "objectID": "prediction.html#locations-in-different-data-frames",
    "href": "prediction.html#locations-in-different-data-frames",
    "title": "5  Prediction",
    "section": "5.4 Locations in Different Data Frames",
    "text": "5.4 Locations in Different Data Frames\nIn the code above, the sulfate concentration that we wished to predict was at a spatial location in the sulfate_na data frame. But, sometimes the spatial location (or spatial locations) that we want to predict at are in a different data frame. For example, suppose that the first seven rows in the sulfate data frame have missing sulfate values and are stored in their own data frame:\n\nsulfate_all_na &lt;- sulfate |&gt; slice(1:7) |&gt; select(-sulfate)\n\nA data frame for the other 190 locations can be made with:\n\nsulfate_complete &lt;- sulfate |&gt; slice(-(1:7))\n\nIf we want to predict the sulfate concentration at the locations in sulfate_all_na, we can use the newdata argument to augment(), assuming that newdata has the same structure as the data used to fit the model.\n\nmod_sulf_more_na &lt;- splm(sulfate ~ 1, data = sulfate_complete,\n                         spcov_type = \"gaussian\")\naugment(mod_sulf_more_na, newdata = sulfate_all_na)\n## Simple feature collection with 7 features and 1 field\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -1437776 ymin: 1080571 xmax: 914593.6 ymax: 1568022\n## Projected CRS: NAD83 / Conus Albers\n## # A tibble: 7 × 2\n##   .fitted           geometry\n## *   &lt;dbl&gt;        &lt;POINT [m]&gt;\n## 1    15.7 (817738.8 1080571)\n## 2    16.8 (914593.6 1295545)\n## 3    16.7 (359574.1 1178228)\n## 4    16.3 (265331.9 1239089)\n## 5    17.9 (304528.8 1453636)\n## 6    16.0 (162932.8 1451625)\n## # ℹ 1 more row\n\nWe obtain predictions for sulfate concentration at these 7 locations based on the model fit with the sulfate_complete data in the .fitted column of the output. We can also obtain prediction intervals with an additional interval argument:\n\naugment(mod_sulf_more_na, newdata = sulfate_all_na, interval = \"prediction\",\n        level = 0.95)\n## Simple feature collection with 7 features and 3 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -1437776 ymin: 1080571 xmax: 914593.6 ymax: 1568022\n## Projected CRS: NAD83 / Conus Albers\n## # A tibble: 7 × 4\n##   .fitted .lower .upper           geometry\n## *   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;        &lt;POINT [m]&gt;\n## 1    15.7   8.04   23.4 (817738.8 1080571)\n## 2    16.8   9.18   24.4 (914593.6 1295545)\n## 3    16.7   9.07   24.4 (359574.1 1178228)\n## 4    16.3   8.60   24.0 (265331.9 1239089)\n## 5    17.9  10.2    25.5 (304528.8 1453636)\n## 6    16.0   8.24   23.7 (162932.8 1451625)\n## # ℹ 1 more row\n\nUsing augment() instead of predict() is particularly useful if there are a lot of spatial locations that you would like to predict at and if you plan on constructing any plots of the predictions."
  },
  {
    "objectID": "logistic-modeling.html#data-introduction-moose-data",
    "href": "logistic-modeling.html#data-introduction-moose-data",
    "title": "6  Spatial Logistic Regression",
    "section": "6.1 Data Introduction: Moose Data",
    "text": "6.1 Data Introduction: Moose Data\nThe moose data set from the spmodel package contains information on 218 spatial locations in Alaska. For each location, we have the following variables:\n\nelev, the elevation at the spatial location.\nstrat, a pre-survey stratification variable that is either L (Low) or M (Medium).\npresence, whether or not moose were present at that location (0 for absent and 1 for present).\ngeometry, a point-level geometry column giving the x and y coordinates of each spatial location.\n\n\nmoose\n## Simple feature collection with 218 features and 4 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: 269085 ymin: 1416151 xmax: 419976.2 ymax: 1541763\n## Projected CRS: NAD83 / Alaska Albers\n## First 10 features:\n##        elev strat count presence                 geometry\n## 1  468.9167     L     0        0 POINT (293542.6 1541016)\n## 2  362.3125     L     0        0 POINT (298313.1 1533972)\n## 3  172.7500     M     0        0 POINT (281896.4 1532516)\n## 4  279.6250     L     0        0 POINT (298651.3 1530264)\n## 5  619.6000     L     0        0 POINT (311325.3 1527705)\n## 6  164.1250     M     0        0 POINT (291421.5 1518398)\n## 7  163.5000     M     0        0 POINT (287298.3 1518035)\n## 8  186.3500     L     0        0 POINT (279050.9 1517324)\n## 9  362.3125     L     0        0 POINT (346145.9 1512479)\n## 10 430.5000     L     0        0 POINT (321354.6 1509966)\n\nOur goal is to construct a spatial model for moose presence, perhaps using elev and strat as predictors in the model. Note that, because presence is binary (moose are either present or absent), a spatial model with normally distributed random errors is not appropriate. But, before we get into any modeling, we first construct a plot of the data:\n\nggplot(data = moose, aes(colour = presence)) +\n  geom_sf() +\n  theme_void() +\n  scale_colour_viridis_d()\n\n\n\n\nWe see from this plot that there are slightly more locations with moose present than there are with moose absent. And, there appears to be some evidence of spatial correlation: locations that are near each other in space seem to be more similar than locations that are further apart. We also clearly see that the sampled locations were not randomly selected, as there are a lot of locations in the eastern part of the plot. Recall from Chapter 4 that random selection of sites is not a strict assumption for the spatial model. If wildlife managers are most interested in the eastern region of the study area, it is perfectly reasonable to concentrate more samples in that region."
  },
  {
    "objectID": "logistic-modeling.html#non-spatial-logistic-background",
    "href": "logistic-modeling.html#non-spatial-logistic-background",
    "title": "6  Spatial Logistic Regression",
    "section": "6.2 Non-spatial Logistic Background",
    "text": "6.2 Non-spatial Logistic Background\nBecause presence is binary, let’s define \\(Y_i\\) as\n\\[\nY_i =\n\\begin{cases}\n1, \\text{ moose are present at location i} \\\\\n0, \\text{ moose are absent at location i}\n\\end{cases}\n\\tag{6.1}\\]\n\\(Y_i\\) is often called a Bernoulli random variable: a variable that takes on a 1 with a certain probability \\(\\pi\\) and a 0 with probability \\(1 - \\pi\\). Note that fitting a spatial model from Chapter 3 no longer makes sense here: if the errors are normally distributed, then the right-hand side of the modeling equation in the form of Equation 4.1 cannot be equal to only the values 0 or 1.\nIn a non-spatial context, we can fit a standard logistic regression model for the binary response variable presence. To do so, we model the log odds that moose are present:\n\\[\n\\text{logodds}(\\pi_i) = \\text{log}\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\beta_0 + \\beta_1 elev_i + \\beta_2 strat_i,\n\\tag{6.2}\\]\nwhere \\(\\pi_i\\) is the probability that \\(Y_i\\) is equal to 1 (and therefore, moose are present at location \\(i\\)) and \\(elev_i\\) and \\(strat_i\\) are the elevation and stratum for location \\(i\\).\nFitting this non-spatial model in R, we can use the glm() function with the \"binomial\" family:\n\nmod_nonspatial &lt;- glm(presence ~ elev + strat, data = moose, family = \"binomial\")\nmod_nonspatial |&gt; summary()\n## \n## Call:\n## glm(formula = presence ~ elev + strat, family = \"binomial\", data = moose)\n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(&gt;|z|)   \n## (Intercept) -0.4247154  0.4208304  -1.009  0.31286   \n## elev        -0.0003173  0.0018607  -0.171  0.86459   \n## stratM       0.8069702  0.2905887   2.777  0.00549 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 302.19  on 217  degrees of freedom\n## Residual deviance: 293.75  on 215  degrees of freedom\n## AIC: 299.75\n## \n## Number of Fisher Scoring iterations: 4\n\nInterpretations of these coefficients are all on the log odds scale. For example, we estimate that the log odds of moose being present on a medium stratum site is 0.807 units more than the log odds of moose being present on a low stratum site, holding elevation constant. Or, we can interpret \\(e^{0.807} = 2.24\\) on the odds scale: we estimate that the odds of moose being present on a medium stratum site are 2.24 times the odds of moose being present on a low stratum site. Because most people have a much easier time thinking about probabilities than they do odds or log odds, these interpretations are only mildly useful."
  },
  {
    "objectID": "logistic-modeling.html#spatial-logistic-regression",
    "href": "logistic-modeling.html#spatial-logistic-regression",
    "title": "6  Spatial Logistic Regression",
    "section": "6.3 Spatial Logistic Regression",
    "text": "6.3 Spatial Logistic Regression\nThe model above assumes that, after accounting for the elev and strat predictors, the bernoulli response variables are spatially independent. How can we extend this model to allow for spatial correlation? We simply add the appropriate error terms from Equation 4.1 in Chapter 3 to Equation 6.2. In doing so, we model spatial correlation on the log odds scale:\n\\[\n\\text{logodds}(\\pi_i) = \\text{log}\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\beta_0 + \\beta_1 elev_i + \\beta_2 strat_i + \\tau_i + \\epsilon_i,\n\\tag{6.3}\\]\nwhere\n\n\\(\\epsilon_i\\) has mean 0, variance \\(\\sigma^2_{ie}\\) and the correlation between \\(\\epsilon_i\\) and \\(\\epsilon_j\\) is equal to 0 for all \\(i \\neq j\\),\n\\(\\tau_i\\) has mean 0, variance \\(\\sigma^2_{de}\\), and the correlation between \\(\\tau_i\\) and \\(\\tau_j\\) can be modeled with a spatial correlation function, like the exponential, gaussian, etc.\nall \\(\\tau\\)’s are independent of all \\(\\epsilon\\)’s.\n\nAgain, the spatial covariance is modeled on the log odds scale, not on the response scale. Therefore, it is incorrect to say that\n\\[\n\\text{cov}(Y_i, Y_j) = \\text{cov}(\\epsilon_i, \\epsilon_j) + \\text{cov}(\\tau_i, \\tau_j).\n\\]\nInstead, we model the spatial correlation on the log odds scale (with an exponential correlation function in the equation below):\n\\[\n\\text{cov}\\left(\\text{log}\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right), \\text{log}\\left(\\frac{\\pi_j}{1 - \\pi_j}\\right)\\right) = \\text{cov}(\\epsilon_i, \\epsilon_j) + \\text{cov}(\\tau_i, \\tau_j) =\n\\begin{cases}\n\\sigma_{ie}^2 + \\sigma_{de}^2 e^{\\frac{-h_{ij}}{\\phi}}, & h_{ij} = 0\\\\\n\\sigma_{de}^2 e^{\\frac{-h_{ij}}{\\phi}}, & h_{ij} &gt; 0\n\\end{cases}\n\\]\nAs we can see from the equation above, modeling the spatial covariance on the log odds scale makes interpretation of the estimated covariance parameters tougher. We can still construct a spatial covariance plot to assess how covariance decays with distance, but the raw values of covariance are a lot harder to understand: these are on the log-odds scale now, which is not as easy to think about as the response variable scale was in Chapter 3.\nBefore we discuss interpretations, let’s fit the model with the spglm() function. Note that the syntax of spglm() is identical to that of glm(), except that we add an extra argument to specify the spcov_type we want to use:\n\nmod_spatial &lt;- spglm(presence ~ elev + strat, data = moose, family = \"binomial\",\n                     spcov_type = \"exponential\")\nmod_spatial |&gt; summary()\n## \n## Call:\n## spglm(formula = presence ~ elev + strat, family = \"binomial\", \n##     data = moose, spcov_type = \"exponential\")\n## \n## Deviance Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.7535 -0.8005  0.3484  0.7893  1.5797 \n## \n## Coefficients (fixed):\n##              Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept) -2.465713   1.486212  -1.659 0.097104 .  \n## elev         0.006036   0.003525   1.712 0.086861 .  \n## stratM       1.439273   0.420591   3.422 0.000622 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Pseudo R-squared: 0.06275\n## \n## Coefficients (exponential spatial covariance):\n##        de        ie     range \n## 5.145e+00 1.294e-03 4.199e+04 \n## \n## Coefficients (Dispersion for binomial family):\n## dispersion \n##          1\n\nWe can interpret the estimated fixed effects in a similar fashion to how they are interpreted in the standard logistic regression model. For example, we estimate that the log odds of moose presence on a medium stratum site are 1.44 units higher than the log odds of moose presence on a low stratum site, holding elevation constant. Or, the odds of moose presence on a medium stratum site are estimated to be \\(e^{1.44} = 4.22\\) times higher than the odds of moose presence on a low stratum site, holding elevation constant. There is also strong evidence of an association between stratum and moose presence (p-value = 0.000622) while there is only marginal evidence of association between elevation and moose presence (p-value = 0.087).\nIn the summary output, we can also obtain estimates of the spatial covariance parameters: \\(\\hat{\\sigma}^2_{de} = 5.145\\), \\(\\hat{\\sigma}^2_{ie} = 0.001294\\), and \\(\\hat{\\phi} = 41990\\). A plot of the covariance on the log odds scale is given below.\n\n\n\n\n\nFrom the plot, we do see evidence of correlation in the log odds of moose presence, as we expected from looking at the plot of the bernoulli presences in the original data.\nMuch of what was discussed in earlier sections of these materials pertaining to spatial models with normally distributed random errors also applies to spatial logistic regression models. For example, we can obtain confidence intervals for the fixed effects coefficients:\n\ntidy(mod_spatial, conf.int = TRUE, conf.level = 0.95)\n## # A tibble: 3 × 7\n##   term        estimate std.error statistic  p.value  conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept) -2.47      1.49        -1.66 0.0971   -5.38        0.447 \n## 2 elev         0.00604   0.00353      1.71 0.0869   -0.000873    0.0129\n## 3 stratM       1.44      0.421        3.42 0.000622  0.615       2.26\n\nWe can glance at the model for some quick summary statistics:\n\nglance(mod_spatial)\n## # A tibble: 1 × 10\n##       n     p  npar value   AIC  AICc   BIC logLik deviance pseudo.r.squared\n##   &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;\n## 1   218     3     3  676.  682.  683.  693.  -338.     176.           0.0627\n\nAnd, we can augment the model for model diagnostics that we can use to make diagnostic plots:\n\naugment(mod_spatial)\n## Simple feature collection with 218 features and 8 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: 269085 ymin: 1416151 xmax: 419057.4 ymax: 1541016\n## Projected CRS: NAD83 / Alaska Albers\n## # A tibble: 218 × 9\n##   presence  elev strat .fitted .resid    .hat  .cooksd .std.resid\n## * &lt;fct&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n## 1 0         469. L       -1.95 -0.516 0.0476  0.00465      -0.528\n## 2 0         362. L       -2.70 -0.361 0.0123  0.000548     -0.363\n## 3 0         173. M       -1.96 -0.514 0.00455 0.000405     -0.516\n## 4 0         280. L       -3.15 -0.290 0.00413 0.000117     -0.291\n## 5 0         620. L       -1.19 -0.728 0.168   0.0427       -0.798\n## 6 0         164. M       -1.71 -0.576 0.00534 0.000598     -0.578\n## # ℹ 212 more rows\n## # ℹ 1 more variable: geometry &lt;POINT [m]&gt;\n\nNote that, in the augmented output, the .fitted column gives fitted values on the log odds scale. If we want these to be on the probability scale, we can either add an argument to augment:\n\naugment(mod_spatial, type.predict = \"response\")\n## Simple feature collection with 218 features and 8 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: 269085 ymin: 1416151 xmax: 419057.4 ymax: 1541016\n## Projected CRS: NAD83 / Alaska Albers\n## # A tibble: 218 × 9\n##   presence  elev strat .fitted .resid    .hat  .cooksd .std.resid\n## * &lt;fct&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n## 1 0         469. L      0.125  -0.516 0.0476  0.00465      -0.528\n## 2 0         362. L      0.0630 -0.361 0.0123  0.000548     -0.363\n## 3 0         173. M      0.124  -0.514 0.00455 0.000405     -0.516\n## 4 0         280. L      0.0412 -0.290 0.00413 0.000117     -0.291\n## 5 0         620. L      0.233  -0.728 0.168   0.0427       -0.798\n## 6 0         164. M      0.153  -0.576 0.00534 0.000598     -0.578\n## # ℹ 212 more rows\n## # ℹ 1 more variable: geometry &lt;POINT [m]&gt;\n\nOr, we can backtransform manually using the fact that:\n\\[\n\\pi_i = \\frac{e^{\\text{logodds}(\\pi_i)}}{1 + e^{\\text{logodds}(\\pi_i)}}\n\\tag{6.4}\\]\n\naugment(mod_spatial) |&gt; mutate(.fitted_back = exp(.fitted) / (1 + exp(.fitted))) |&gt;\n  relocate(.fitted_back)\n## Simple feature collection with 218 features and 9 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: 269085 ymin: 1416151 xmax: 419057.4 ymax: 1541016\n## Projected CRS: NAD83 / Alaska Albers\n## # A tibble: 218 × 10\n##   .fitted_back presence  elev strat .fitted .resid    .hat  .cooksd .std.resid\n##          &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n## 1       0.125  0         469. L       -1.95 -0.516 0.0476  0.00465      -0.528\n## 2       0.0630 0         362. L       -2.70 -0.361 0.0123  0.000548     -0.363\n## 3       0.124  0         173. M       -1.96 -0.514 0.00455 0.000405     -0.516\n## 4       0.0412 0         280. L       -3.15 -0.290 0.00413 0.000117     -0.291\n## 5       0.233  0         620. L       -1.19 -0.728 0.168   0.0427       -0.798\n## 6       0.153  0         164. M       -1.71 -0.576 0.00534 0.000598     -0.578\n## # ℹ 212 more rows\n## # ℹ 1 more variable: geometry &lt;POINT [m]&gt;"
  },
  {
    "objectID": "logistic-modeling.html#prediction",
    "href": "logistic-modeling.html#prediction",
    "title": "6  Spatial Logistic Regression",
    "section": "6.4 Prediction",
    "text": "6.4 Prediction\nUsing our fitted spatial logistic regression model, we can also obtain predicted probabilities of moose presence at spatial locations not in the moose data frame. The moose_preds data frame in the spmodel package contains 100 locations in the region of interest that were not sampled for moose presence.\n\nmoose_preds\n## Simple feature collection with 100 features and 2 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: 269085 ymin: 1416151 xmax: 419976.2 ymax: 1541763\n## Projected CRS: NAD83 / Alaska Albers\n## First 10 features:\n##        elev strat                 geometry\n## 1  143.4000     L POINT (401239.6 1436192)\n## 2  324.4375     L POINT (352640.6 1490695)\n## 3  158.2632     L POINT (360954.9 1491590)\n## 4  221.3125     M POINT (291839.8 1466091)\n## 5  208.6875     M POINT (310991.9 1441630)\n## 6  218.3333     L POINT (304473.8 1512103)\n## 7  126.8125     L POINT (339011.1 1459318)\n## 8  122.0833     L POINT (342827.3 1463452)\n## 9  191.0000     L POINT (284453.8 1502837)\n## 10 105.3125     L POINT (391343.9 1483791)\n\nWe can make a plot of these locations overlaid on the sampled locations with:\n\nggplot(data = moose, aes(colour = presence)) +\n  geom_sf() +\n  theme_void() +\n  scale_colour_viridis_d() +\n  geom_sf(data = moose_preds, colour = \"grey\")\n\n\n\n\nConceptually, the predictions for the log odds of moose presence are found in a similar fashion to the way that the predictions were calculated from a model with normally distributed random errors in Chapter 5. That is, for a particular spatial location we wish to predict for, we use the estimated log odds of presence for locations that we did observe to predict our location of interest’s log odds of presence. Locations that are closer generally have more weight than locations that are further away when there is evidence of spatial correlation.\nWe can either use augment() or predict() to obtain the predicted log odds of moose presence at these 100 unsampled spatial locations. If the locations we wish to predict at are not in the data frame used to fit the model, then we provide a newdata argument that is a data frame (or, in this case, an sf object) that has the same structure as the data used to fit the model (including values for all predictors in the model and the appropriate coordinates).\n\naugment(mod_spatial, newdata = moose_preds)\n## Simple feature collection with 100 features and 3 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: 269386.2 ymin: 1418453 xmax: 419976.2 ymax: 1541763\n## Projected CRS: NAD83 / Alaska Albers\n## # A tibble: 100 × 4\n##    elev strat .fitted           geometry\n## * &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;        &lt;POINT [m]&gt;\n## 1  143. L      0.0666 (401239.6 1436192)\n## 2  324. L     -0.791  (352640.6 1490695)\n## 3  158. L     -1.60   (360954.9 1491590)\n## 4  221. M     -0.832  (291839.8 1466091)\n## 5  209. M      1.38   (310991.9 1441630)\n## 6  218. L     -2.59   (304473.8 1512103)\n## # ℹ 94 more rows\npredict(mod_spatial, newdata = moose_preds)\n##           1           2           3           4           5           6 \n##  0.06664165 -0.79069107 -1.60387940 -0.83159357  1.38183928 -2.58965413 \n##           7           8           9          10          11          12 \n## -2.72839733 -2.32496923 -1.17473690 -0.90466930  0.05799546  0.99056609 \n##          13          14          15          16          17          18 \n## -0.23933323 -0.71908105 -1.46827406 -1.55190915 -1.24303941 -1.52095927 \n##          19          20          21          22          23          24 \n## -1.33020240 -1.34270385 -1.18290244 -1.18140675 -0.65388088 -1.03825533 \n##          25          26          27          28          29          30 \n## -1.50181199 -1.40079058  1.38943270 -3.01724805 -0.03723394 -1.11569821 \n##          31          32          33          34          35          36 \n##  0.45816827 -1.57152295 -0.63546548 -1.27432691 -0.76083448 -2.30737472 \n##          37          38          39          40          41          42 \n## -1.14293043 -1.71496112 -1.57409265 -2.86157515  0.24048281 -3.00741514 \n##          43          44          45          46          47          48 \n## -1.24239585 -0.50836234 -2.84875530 -0.92482387 -1.98918604 -1.98764398 \n##          49          50          51          52          53          54 \n## -1.17104433 -1.85583242 -1.45015937 -0.85817042  0.51109213 -1.77286716 \n##          55          56          57          58          59          60 \n## -0.58274230 -1.18147493 -2.03679898 -1.32568862 -3.06225494 -0.80315086 \n##          61          62          63          64          65          66 \n## -0.80596369 -1.36877976 -1.06715436  0.45234920 -1.79642815 -2.43463712 \n##          67          68          69          70          71          72 \n## -1.44374223  0.55547296 -2.52374613 -1.32588753 -2.00112730 -2.29721996 \n##          73          74          75          76          77          78 \n## -0.51091841 -1.33332860 -2.26834806 -2.01935161 -2.08573552 -2.02644905 \n##          79          80          81          82          83          84 \n## -1.52701892 -0.31166064 -2.22601376 -0.14443115 -2.09258984 -2.25393805 \n##          85          86          87          88          89          90 \n## -1.50149150 -1.19017489 -1.92658815 -2.54556422 -1.37429146 -3.00392216 \n##          91          92          93          94          95          96 \n## -1.38675655 -1.72255710 -1.09102564 -0.74867508 -1.34594296  1.04616966 \n##          97          98          99         100 \n## -0.24654727 -1.63627214 -1.06809835  0.01457089\n\nWhile augment() returns the predicted log odds of presence in a data frame, predict() returns the predicted log odds of presence as a vector. If we want the predictions on the probability scale, we can either backtransform the log odds manually using Equation 6.4, or, we can provide a type.predict = \"response\" argument to augment() or predict():\n\n## shown for augment() only\naug_pred &lt;- augment(mod_spatial, newdata = moose_preds, type.predict = \"response\")\n\nBelow, we plot the predicted probabilities:\n\nggplot(data = aug_pred) +\n  geom_sf(aes(colour = .fitted)) +\n  theme_void() +\n  scale_colour_viridis_c()\n\n\n\n\nThe predicted probabilities do seem reasonable given the plot of the original data: predicted probabilities of presence tend to be higher in the eastern and southeastern region of the study area where a lot of moose presences were observed and lower in the northwestern region of the study area where a lot of moose absences were observed.\nIn this section, we focused only on a spatial extension to logistic regression. Spatial extensions to other generalized linear models, like Poisson regression, Gamma regression, etc. are developed similarly: the spatial modeling is performed on the “link” scale (logit in the case of logistic regression), and these models can be fit by changing the family argument of spglm() to \"poisson\", \"gamma\", etc."
  },
  {
    "objectID": "areal-modeling.html#data-introduction-countries-in-africa",
    "href": "areal-modeling.html#data-introduction-countries-in-africa",
    "title": "7  Areal Modeling",
    "section": "7.1 Data Introduction: Countries in Africa",
    "text": "7.1 Data Introduction: Countries in Africa\nTo motivate the use of spatial autoregressive models, consider the African countries in the world data set from the spData package. The world data set has a few variables of interest for each country, but we will focus on lifeExp, the life expectancy at birth in the year 2014. For now, we filter out the island country of \"Madagascar\", though we will add this country back in at the end of the section. And, the plot below gives a black outline around \"Tanzania\", a country on the eastern coast of Africa. We will use \"Tanzania\" as an example country throughout this section.\n\nafrica_cont &lt;- world |&gt; filter(continent == \"Africa\") |&gt;\n  filter(name_long != \"Madagascar\")\n\nggplot(data = africa_cont, aes(fill = lifeExp)) +\n  geom_sf() +\n  geom_sf(data = africa_cont |&gt; filter(name_long == \"Tanzania\"),\n          colour = \"black\") +\n  theme_void() +\n  scale_fill_viridis_c()\n\n\n\n\nLet’s take note of a few things about this data set. First, the entire structure of the data set is different than all of the other data sets we have seen in previous sections: the data is now areal polygons, not points. Note in the printout of the africa_cont sf object that the Geometry type is now MULTIPOLYGON, indicating that at least some countries in the original world data set are made up of more than one polygon (New Zealand is one such example, as the country is made up of multiple islands, each its own polygon).\n\nafrica_cont\n## Simple feature collection with 50 features and 10 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -17.62504 ymin: -34.81917 xmax: 51.13387 ymax: 37.34999\n## Geodetic CRS:  WGS 84\n## # A tibble: 50 × 11\n##   iso_a2 name_long  continent region_un subregion type  area_km2     pop lifeExp\n## * &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n## 1 TZ     Tanzania   Africa    Africa    Eastern … Sove…  932746.  5.22e7    64.2\n## 2 EH     Western S… Africa    Africa    Northern… Inde…   96271. NA         NA  \n## 3 CD     Democrati… Africa    Africa    Middle A… Sove… 2323492.  7.37e7    58.8\n## 4 SO     Somalia    Africa    Africa    Eastern … Sove…  484333.  1.35e7    55.5\n## 5 KE     Kenya      Africa    Africa    Eastern … Sove…  590837.  4.60e7    66.2\n## 6 SD     Sudan      Africa    Africa    Northern… Sove… 1850886.  3.77e7    64.0\n## # ℹ 44 more rows\n## # ℹ 2 more variables: gdpPercap &lt;dbl&gt;, geom &lt;MULTIPOLYGON [°]&gt;\n\nThe geom column in africa_cont contains “directions” for how R should draw each polygon by providing a long list of coordinates to connect with lines to form the polygons for each country.\nAnalyzing spatial data where each observation is collected at a particular point in space is sometimes called geostatistical analysis. We have performed geostatistical analysis for all examples prior to this one. With this data set, life expectancy is collected across an entire area (a country) so we now have areal data.\nSometimes, geostatistical analysis is still performed on areal data, by using the centroid of each polygon as a substitute for “point-level” coordinates. The moose data set used in Chapter 6 is one such example: moose presence is observed on a polygonal area but the data we have only contains the centroid of each polygon. More often, however, the autoregressive models that we discuss next are used for this type of data if we have information on the entire polygon, not just the centroid.\nWe also see from the plot that there are two countries in Africa (Western Sahara and Somaliland) for which we do not have values of life expectancy in the data:\n\nafrica_cont |&gt; filter(is.na(lifeExp)) |&gt; pull(name_long)\n## [1] \"Western Sahara\" \"Somaliland\"\n\nWe may eventually be interested in predicting values of life expectancy for these two countries.\nFinally, we see that there does appear to be some spatial correlation in life expectancy: countries in the northern part of Africa tend to have higher life expectancies than other countries in the continent."
  },
  {
    "objectID": "areal-modeling.html#the-basic-idea-of-an-autoregressive-model",
    "href": "areal-modeling.html#the-basic-idea-of-an-autoregressive-model",
    "title": "7  Areal Modeling",
    "section": "7.2 The Basic Idea of an Autoregressive Model",
    "text": "7.2 The Basic Idea of an Autoregressive Model\nHow does the change in data structure affect how we formulate a spatial model? The most common methods to analyze areal data are autoregressive models, which typically use a neighborhood structure instead of distance to model spatial covariance.\nHow the structure of the neighborhood is defined is up to the analyst. Most commonly, two polygons are considered to be “neighbors” if the polygons share a boundary. For example, consider Tanzania, the country with the black outline in the earlier plot of the data. Tanzania has eight neighbors, which are shown in the plot below as countries with asterisks. These eight neighbors are the only countries in the data set that share a border with Tanzania.\n\n\n\n\n\nThere are 50 countries in africa_cont. We can record whether or not each country is a neighbor of Tanzania with a vector of zeroes and ones, where a zero denotes that the country is not a neighbor with Tanzania and a one denotes that the country is a neighbor with Tanzania. For example, of the first 10 countries, only the Democratic Republic of Congo (the third country) and Kenya (the fifth country) are neighbors with Tanzania (Tanzania is not a “neighbor” to itself). So, we would start our recording with 0, 0, 1, 0, 1, 0, 0, 0, 0, 0. Going through the rest of the data in this way, we end up with:\n\n##  [1] 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0\n## [39] 0 0 0 0 0 0 0 0 0 1 1 0\n\nIn the printout, there are eight “ones,” corresponding to the eight neighbors of Tanzania.\nWe can then repeat this for every country in the data set and store the results in a weight matrix that has 50 rows and 50 columns, where the \\(i^{th}\\) row gives the neighbors of the \\(i^{th}\\) country. The first 10 rows of the weight matrix are shown below, where each of the ten rows corresponds to one of the first ten countries in africa_cont. The “dots” correspond to zeroes so that the printout is a little easier to read:\n\n## 10 x 50 sparse Matrix of class \"dgCMatrix\"\n##                                                                                \n##  [1,] . . 1 . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1 1 . . 1\n##  [2,] . . . . . . . . . . . . . . 1 . . . . . . . . . . . . . . . . . . . . . .\n##  [3,] 1 . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1 . . 1 . . . 1 1\n##  [4,] . . . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n##  [5,] 1 . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n##  [6,] . . . . . . 1 . . . . . . . . . . . . . . . . . . . . 1 . . . . . . . . .\n##  [7,] . . . . . 1 . . . . . . . . . . 1 . 1 . . . . . . . . 1 . . . . . . . . .\n##  [8,] . . . . . . . . 1 1 1 1 . . . . . . . . . . . . . . . . . . . . . 1 1 . .\n##  [9,] . . . . . . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n## [10,] . . . . . . . 1 . . 1 . . . . . . . . . . . . . . . . . . . . 1 . 1 . . .\n##                                \n##  [1,] . . . . . . . . . . 1 1 .\n##  [2,] . . 1 . 1 . . . . . . . .\n##  [3,] . . . . . . . . . . 1 1 1\n##  [4,] . . . . . . . 1 . 1 . . .\n##  [5,] . . . . . . . 1 . . 1 . 1\n##  [6,] . . . 1 . 1 1 1 . . . . 1\n##  [7,] . . . . . . 1 . . . . . .\n##  [8,] . . . . . . . . . . . . .\n##  [9,] . . . . . . . . . . . . .\n## [10,] . . . . . . . . . . . . .\n\nThe printout tells us, for example, that the ninth country in the data set only has one neighbor (we only see a single 1 in the ninth row) and that this country’s neighbor is the eighth country in the data set (because the 1 is in the 8th column of the matrix printout).\nUnfortunately, gaining intuition about the covariance used in an autoregressive model is much more challenging than it was for geostatistical distance-based models. The reason is that for spatial autoregressive models, we do not use the weight matrix to directly model covariance. Instead, the weight matrix is used to model matrix inverse of the covariance, which is harder to understand. Below, we try to gain some intuition for what the covariance looks like for this particular example and define a matrix inverse.\nWe have mostly avoided matrix notation up to this point, but, for autoregressive models, the notation is a necessity. The covariance matrix, denoted \\(\\boldsymbol{\\Sigma}\\), is a matrix that stores all of the variances and covariances: the variances go on the diagonal of the matrix and the covariances go on the off-diagonal. As a toy example, consider a covariance matrix for just toy observations \\(Y_1, Y_2, Y_3\\):\n\\[\n\\boldsymbol{\\Sigma} = \\begin{pmatrix}\n  3 & 0.4 & 0.6 \\\\\n  0.4 & 2 & 0.9 \\\\\n  0.6 & 0.9 & 2.5\n\\end{pmatrix}\n\\]\nThe covariance matrix says that \\(\\text{var}(Y_1) = 3\\), \\(\\text{var}(Y_2) = 2\\), and \\(\\text{var}(Y_3) = 2.5\\). The matrix also tells us that \\(\\text{cov}(Y_1, Y_2) = \\text{cov}(Y_2, Y_1) = 0.4\\), \\(\\text{cov}(Y_1, Y_3) = \\text{cov}(Y_3, Y_1) = 0.6\\), and \\(\\text{cov}(Y_2, Y_3) = \\text{cov}(Y_3, Y_2) = 0.9\\).\nWe can fit a conditional autoregressive (CAR) model with lifeExp as the response variable and no predictors (code is omitted for now). The model that we are fitting is:\n\\[\nY_i = \\beta_0 + \\tau_i,\n\\]\nwhere \\(Y_i\\) is the average life expectancy in the \\(i^{th}\\) country, \\(\\beta_0\\) is the intercept and \\(\\tau_i\\) is the spatial random error. For a CAR model, the mean of \\(\\tau_i\\) is \\(0\\), the variance of \\(\\tau_i\\) is equal to the value on the \\(i^{th}\\) row and \\(i^{th}\\) column of the covariance matrix \\(\\boldsymbol{\\Sigma}\\), and the covariance of \\(\\tau_i\\) with \\(\\tau_j\\) is equal to the value on the \\(i^{th}\\) row and \\(j^{th}\\) column of the covariance matrix \\(\\boldsymbol{\\Sigma}\\). Additionally, each \\(\\tau_i\\) is often assumed to follow a normal distribution.\nNote that the specification above does not have an independent error term \\(\\epsilon_i\\): one can be included, but many CAR models do not contain an independent error term.\nSo, what exactly is this covariance matrix \\(\\boldsymbol{\\Sigma}\\) in a CAR model? As alluded to earlier, for autoregressive models, we do not model the covariance matrix \\(\\boldsymbol{\\Sigma}\\) directly; instead, we model the inverse of the covariance matrix, sometimes called the precision matrix:\n\\[\n\\boldsymbol{\\Sigma}^{-1} = (\\mathbf{I} - \\rho \\mathbf{W}) / \\sigma^2_{de},\n\\]\nwhere \\(\\mathbf{W}\\) is the weight matrix discussed previously, \\(\\mathbf{I}\\) is a matrix that has all 1’s on the diagonal and 0’s on all of the off-diagonals, \\(\\sigma^2_{de}\\) is the dependent error variance parameter, \\(\\rho\\) is the range parameter, and the \\(-1\\) denotes a matrix inverse. A matrix inverse to a matrix \\(\\mathbf{A}\\) is the matrix such that multiplying \\(\\mathbf{A}\\mathbf{A}^{-1}\\) gives a matrix with all 1’s on the diagonal and 0’s on all of the off-diagonals (the identity matrix). The idea is similar to the inverse of a scalar number: the inverse of the number \\(5\\) for example is equal to \\(\\frac{1}{5}\\) because \\(5 \\cdot \\frac{1}{5} = 1\\).\nThen, the covariance matrix is:\n\\[\n\\boldsymbol{\\Sigma} = \\sigma^2_{de}(\\mathbf{I} - \\rho \\mathbf{W})^{-1}.\n\\]\nAgain, intuition about what this covariance matrix will end up looking like is quite challenging. But, we should expect that:\n\nPolygons that are neighbors should generally have a higher covariance than polygons that are very far away from one another.\nThe covariance between neighboring polygons should be higher for a larger value of \\(\\rho\\) than for a smaller value of \\(\\rho\\).\n\nIn the life expectancy example, \\(\\hat{\\sigma}^2_{de} = 29.39\\) and \\(\\hat{\\rho} = 0.1529\\), yielding the following estimated covariance matrix \\(\\hat{\\boldsymbol{\\Sigma}}\\) (only the first 5 rows and 5 columns are shown in the printout):\n\n##                                   Tanzania Democratic Republic of the Congo\n## Tanzania                         49.035901                        22.650040\n## Democratic Republic of the Congo 22.650040                        52.650026\n## Somalia                           3.093125                         2.891017\n## Kenya                            13.087356                        10.356243\n## Sudan                             4.909081                         8.174690\n##                                    Somalia     Kenya     Sudan\n## Tanzania                          3.093125 13.087356  4.909081\n## Democratic Republic of the Congo  2.891017 10.356243  8.174690\n## Somalia                          33.104975  8.090651  3.188561\n## Kenya                             8.090651 38.204129  5.919240\n## Sudan                             3.188561  5.919240 41.390239\n\nRecall that both the Democratic Republic of the Congo and Kenya are direct neighbors of Tanzania, while the other two countries in the printout are not direct neighbors. As we would expect, we see that the covariance for Tanzania and the Democratic Republic of the Congo and the covariance for Tanzania and Kenya are both larger than the other covariances in the first row. We also see that the covariance for Tanzania and the Democratic Republic of the Congo is larger than the covariance for Tanzania and Kenya, even though both are neighbors. The exact covariance is different because it takes into account the nature of the “connectedness” the Democratic Republic of the Congo and Kenya have with other countries in the data set.\nWe also see that, unlike the geostatistical models we fit to point data, the variances on the diagonal are not all equal. If we plot the diagonal variances vs. the number of neighbors each country has, we end up with the somewhat undesirable property that locations with more neighbors tend to have higher variance:\n\n\n\n\n\nThis is slightly problematic in that we would expect that locations with more neighbors would have a lower overall variance, as we have more information about surrounding life expectancies. To address this issue, what is often done in CAR models is a row standardization of the weight matrix \\(\\mathbf{W}\\). To make \\(\\mathbf{W}\\) row standardized, for each row \\(i\\), we divide by the total number of 1’s (or, the total number of neighbors) for that row. For example, the first row of \\(\\mathbf{W}\\) has eight 1’s, corresponding to the eight neighbors of Tanzania. Therefore, we divide each value in this first row by eight. Only the first three rows of the row-standardized weight matrix are shown below:\n\n## 3 x 50 sparse Matrix of class \"dgCMatrix\"\n##                                                                               \n## [1,] .         . 0.125 . 0.125 . . . . . . . . . .         . . . . . . . . . .\n## [2,] .         . .     . .     . . . . . . . . . 0.3333333 . . . . . . . . . .\n## [3,] 0.1111111 . .     . .     . . . . . . . . . .         . . . . . . . . . .\n##                                                                               \n## [1,] . . .         .         . . 0.1250000 0.125 0.125 . .         0.1250000 .\n## [2,] . . .         .         . . .         .     .     . .         .         .\n## [3,] . . 0.1111111 0.1111111 . . 0.1111111 .     .     . 0.1111111 0.1111111 .\n##                                                                     \n## [1,] . .         . .         . . . . . 0.1250000 0.1250000 .        \n## [2,] . 0.3333333 . 0.3333333 . . . . . .         .         .        \n## [3,] . .         . .         . . . . . 0.1111111 0.1111111 0.1111111\n\nWith \\(\\mathbf{W}\\) row standardized, we also need to multiply the resulting covariance matrix by a symmetry condition matrix \\(\\mathbf{M}\\), which forces the full covariance matrix \\(\\boldsymbol{\\Sigma}\\) to be symmetric:\n\\[\n\\boldsymbol{\\Sigma} = \\sigma^2_{de}(\\mathbf{I} - \\rho \\mathbf{W}_{st})^{-1} \\mathbf{M},\n\\]\nwhere \\(\\mathbf{W}_{st}\\) is the row-standardized weight matrix. If we make another plot of the variances vs. the number of neighbors, we obtain a pattern that is a bit more intuitive: locations with more neighbors tend to have lower overall variance:\n\n\n\n\n\nTherefore, partially for the reason that locations with more neighbors tend to have lower variance if the weight matrix is row-standardized, CAR models are often fit with this row-standardized weight matrix."
  },
  {
    "objectID": "areal-modeling.html#fitting-an-autoregressive-model",
    "href": "areal-modeling.html#fitting-an-autoregressive-model",
    "title": "7  Areal Modeling",
    "section": "7.3 Fitting an Autoregressive Model",
    "text": "7.3 Fitting an Autoregressive Model\nWhile the underlying theory behind autoregressive models is a bit more challenging than the theory behind geostatistical models, the actual model fitting process is quite straightforward. The spautor() function from the spmodel package has similar syntax to splm(). The primary differences are that:\n\nThe data argument in spautor() must be an sf object with POLYGON or MULTIPOLYGON geometry. Alternatively, a user can provide a regular data frame object as data and a weight matrix \\(\\mathbf{W}\\) constructed by hand.\nThe spcov_type argument is either \"car\" (discussed above) or \"sar\", which stands for simultaneous autoregressive model (not discussed).\n\nWith the life expectancy data, fitting a CAR model can be done with:\n\nmod_car &lt;- spautor(lifeExp ~ 1, data = africa_cont, spcov_type = \"car\")\nmod_car\n## \n## Call:\n## spautor(formula = lifeExp ~ 1, data = africa_cont, spcov_type = \"car\")\n## \n## \n## Coefficients (fixed):\n## (Intercept)  \n##        61.2  \n## \n## \n## Coefficients (car spatial covariance):\n##       de     range  \n## 103.3341    0.9016\n\nBy default, row standardization is done on the weight matrix. So, our fitted model is:\n\\[\n\\hat{Y_i} = 61.2\n\\]\nwith\n\\[\n\\boldsymbol{\\hat{\\Sigma}} = 103.33 (\\mathbf{I} - 0.9016 \\mathbf{W}_{st})^{-1} \\mathbf{M},\n\\]\nwhere \\(\\mathbf{W}_{st}\\) is the row-standardized weight matrix.\nWe can look at the row-standardized weight matrix, along with the diagonal elements of the symmetry condition matrix \\(\\mathbf{M}\\) with (output is omitted here for space):\n\nmod_car$W\nmod_car$M\n\nMost of the functions used for geostatistical models on point data can also be applied to autoregressive models on areal data. For example, we can obtain a tidy summary table of output with:\n\ntidy(mod_car)\n## # A tibble: 1 × 5\n##   term        estimate std.error statistic p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1 (Intercept)     61.2      2.19      28.0       0\n\nand we can glance at model summary statistics with:\n\nglance(mod_car)\n## # A tibble: 1 × 10\n##       n     p  npar value   AIC  AICc   BIC logLik deviance pseudo.r.squared\n##   &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;\n## 1    48     1     2  296.  300.  300.  304.  -148.     46.8                0\n\nWe can also obtain diagnostic statistics with:\n\naugment(mod_car)\n## Simple feature collection with 48 features and 6 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -17.62504 ymin: -34.81917 xmax: 51.13387 ymax: 37.34999\n## Geodetic CRS:  WGS 84\n## # A tibble: 48 × 7\n##   lifeExp .fitted .resid   .hat .cooksd .std.resid                          geom\n## *   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;            &lt;MULTIPOLYGON [°]&gt;\n## 1    64.2    61.2   2.96 0.0288 0.0245       0.910 (((33.90371 -0.95, 31.86617 …\n## 2    58.8    61.2  -2.42 0.0308 0.00701     -0.470 (((29.34 -4.499983, 29.27638…\n## 3    55.5    61.2  -5.73 0.0205 0.0278      -1.15  (((41.58513 -1.68325, 41.810…\n## 4    66.2    61.2   5.04 0.0237 0.0396       1.28  (((39.20222 -4.67677, 39.604…\n## 5    64.0    61.2   2.80 0.0267 0.0102       0.610 (((23.88711 8.619775, 24.194…\n## 6    52.2    61.2  -9.00 0.0237 0.0927      -1.95  (((23.83766 19.58047, 19.849…\n## # ℹ 42 more rows"
  },
  {
    "objectID": "areal-modeling.html#prediction",
    "href": "areal-modeling.html#prediction",
    "title": "7  Areal Modeling",
    "section": "7.4 Prediction",
    "text": "7.4 Prediction\nRecall that one of the goals we had at the outset of this section was to make predictions for the average life expectancies for the two countries that had missing values:\n\nafrica_cont |&gt; filter(is.na(lifeExp))\n## Simple feature collection with 2 features and 10 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -17.06342 ymin: 7.99688 xmax: 48.94821 ymax: 27.65643\n## Geodetic CRS:  WGS 84\n## # A tibble: 2 × 11\n##   iso_a2 name_long    continent region_un subregion type  area_km2   pop lifeExp\n## * &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n## 1 EH     Western Sah… Africa    Africa    Northern… Inde…   96271.    NA      NA\n## 2 &lt;NA&gt;   Somaliland   Africa    Africa    Eastern … Inde…  167350.    NA      NA\n## # ℹ 2 more variables: gdpPercap &lt;dbl&gt;, geom &lt;MULTIPOLYGON [°]&gt;\n\nIf there is some spatial correlation in life expectancies, we want to use that information to help inform our predictions for life expectancy in these two countries. We can use the augment() function with a newdata argument to obtain predictions and 95% prediction intervals:\n\naugment(mod_car, newdata = mod_car$newdata, interval = \"prediction\") |&gt;\n  relocate(.fitted, .lower, .upper) \n## Simple feature collection with 2 features and 13 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -17.06342 ymin: 7.99688 xmax: 48.94821 ymax: 27.65643\n## Geodetic CRS:  WGS 84\n## # A tibble: 2 × 14\n##   .fitted .lower .upper iso_a2 name_long     continent region_un subregion type \n##     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;         &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;\n## 1    69.3   57.7   80.8 EH     Western Saha… Africa    Africa    Northern… Inde…\n## 2    60.5   49.0   72.0 &lt;NA&gt;   Somaliland    Africa    Africa    Eastern … Inde…\n## # ℹ 5 more variables: area_km2 &lt;dbl&gt;, pop &lt;dbl&gt;, lifeExp &lt;dbl&gt;,\n## #   gdpPercap &lt;dbl&gt;, geom &lt;MULTIPOLYGON [°]&gt;\n\nThe extra syntax in the newdata argument pulls the newdata object from mod_car, which contains information about the two countries with missing life expectancy values.\nWe can again examine the plot of the original data:\n\nggplot(data = africa_cont, aes(fill = lifeExp)) +\n  geom_sf() +\n  geom_sf(data = africa_cont |&gt; filter(name_long == \"Tanzania\"),\n          colour = \"black\") +\n  theme_void() +\n  scale_fill_viridis_c()\n\n\n\n\nWestern Sahara is a disputed territory in northwest Africa. Because this region is surrounded by countries with relatively high life expectancies, the predicted life expectancy is also on the higher end, at 69.3 years. Somaliland is on the east coast of Africa. Because this area is surrounded by countries with life expectancies close to the median life expectancy, the prediction for the life expectancy in this area is also close to the middle of the life expectancy distribution on the continent at 60.5 years. However, both prediction intervals are quite wide, indicating a large amount of uncertainty in the life expectancies in these two regions.\nNote. At the beginning of the analysis, we removed the island of Madagascar, which has no neighbors because the country does not share a boundary with any other country. If we include Madagascar, we also include an extra covariance parameter that allows Madagascar to have its own variance. The country also then has no covariance with any of the other countries in the data set. The extra variance parameter is called extra in the output:\n\nafrica_mad &lt;- world |&gt; filter(continent == \"Africa\")\nmod_mad &lt;- spautor(lifeExp ~ 1, data = africa_mad, spcov_type = \"car\")\nmod_mad\n## \n## Call:\n## spautor(formula = lifeExp ~ 1, data = africa_mad, spcov_type = \"car\")\n## \n## \n## Coefficients (fixed):\n## (Intercept)  \n##       62.64  \n## \n## \n## Coefficients (car spatial covariance):\n##       de     range     extra  \n## 100.1785    0.9181    9.6585\n\nIn this section, we have provided a brief introduction to autoregressive models. To learn more about these models, we recommend starting with a basic understanding of matrix algebra along with an understanding of conditional probability, which is used to help formulate the models. With these in hand, other online resources can provide more depth with the theory behind these models."
  }
]